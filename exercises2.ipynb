{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_data = pd.read_csv(\"./seOMXlogreturns2012to2014.csv\")\n",
    "T = observation_data.shape[0]\n",
    "observation_data = observation_data.to_numpy()[:, 0]\n",
    "observation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0.98\n",
    "sigma = 0.16\n",
    "betas = np.linspace(0.1, 2, 5)\n",
    "\n",
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bootstrap Particle Filter\n",
    "loglikelihood = []\n",
    "\n",
    "for beta in tqdm(betas):\n",
    "    loglikelihood_ = []\n",
    "\n",
    "    for repeat in range(10):\n",
    "\n",
    "        loglikelihood__ = 0\n",
    "        initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "        weights = [np.array([1/N] * N)] + [None] * T\n",
    "        particles = [initial_particle_dist.rvs(N)] + [None] * T  # draw initial particles\n",
    "        mean_observation = [None] * T\n",
    "        prediction = [None] * T\n",
    "        marginal_filtering = [None] * T\n",
    "\n",
    "        for t in range(T):\n",
    "            # RESAMPLE\n",
    "            ancestor_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "\n",
    "            # PROPAGATE\n",
    "            # state\n",
    "            proposal_dist = scipy.stats.norm(phi * particles[t][ancestor_indices], sigma)\n",
    "            particles[t+1] = proposal_dist.rvs()\n",
    "\n",
    "            # measurement\n",
    "            measurement_dist = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(particles[t+1])))\n",
    "            # mean observation\n",
    "            mean_observation[t] = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(np.mean(particles[t+1])))).rvs()\n",
    "\n",
    "            # WEIGHT\n",
    "            log_weights_unnorm = measurement_dist.logpdf(observation_data[t])\n",
    "            weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "            weights[t+1] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "            prediction[t] = np.mean(particles[t])\n",
    "            marginal_filtering[t] = np.sum(weights[t] * particles[t])\n",
    "\n",
    "            loglikelihood__ += np.log(np.sum(weights_unnorm)) - np.log(N) + np.max(log_weights_unnorm)\n",
    "\n",
    "        loglikelihood_.append(loglikelihood__)\n",
    "    \n",
    "    loglikelihood.append(loglikelihood_)\n",
    "\n",
    "    weights = np.array(weights[:-1])\n",
    "    particles = np.array(particles[:-1])\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    prediction = np.array(prediction)\n",
    "    marginal_filtering = np.array(marginal_filtering)\n",
    "\n",
    "loglikelihood = np.array(loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(loglikelihood.T, positions=betas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(np.max(loglikelihood, axis=1))\n",
    "best_beta = betas[idx]\n",
    "\n",
    "plt.scatter(best_beta, np.max(loglikelihood, axis=1)[idx], c='r', label=f\"Optimal beta = {best_beta}\")\n",
    "plt.plot(betas, np.mean(loglikelihood, axis=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(best_beta, np.max(loglikelihood, axis=1)[idx], c='r', label=f\"Optimal beta = {best_beta}\")\n",
    "plt.plot(betas, loglikelihood);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing $N$ yields lower variance.\n",
    "\n",
    "Inreasing $T$ yields lower variance since the influence of the likelihood of the first few timesteps is artificially bad due to the filter not having converged. Increasing $T$ reduces the influence of these first timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) This is not possible because the observation noise is not Gaussian which makes the $p(y_t|x_t)$ not **conjugate** to $p(x_t|x_{t-1})$ (required for Fully Adapted Particle Filter). We could use a Partially Adapted Particle Filter instead with an approximation of the two.\n",
    "\n",
    "(ii) This is a Gaussian model and hence OK to implement.\n",
    "\n",
    "(iii) Cannot implement fully adapted filter since the noise is added inside the cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully adapted particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Adapted Particle Filter\n",
    "\n",
    "N = 500\n",
    "\n",
    "loglikelihood = 0\n",
    "\n",
    "initial_particle_dist = scipy.stats.norm(1, 1)  # the actual best initial distribution\n",
    "weights = [np.array([1/N] * N)] + [None] * T  # these are nu weights\n",
    "particles = [None] * T + [initial_particle_dist.rvs(N)]  # draw initial particles - put at index -1\n",
    "mean_observation = [None] * T  # p(y_t|x_t)\n",
    "std_observation = [None] * T\n",
    "mean_state_prediction = [None] * T  # p(x_t|x_t-1)\n",
    "std_state_prediction = [None] * T\n",
    "mean_marginal_filtering = [None] * T  # p(x_t|x_t-1, y_t)\n",
    "std_marginal_filtering = [None] * T\n",
    "\n",
    "for t in tqdm(range(T)):\n",
    "    # RESAMPLE\n",
    "    # measurement\n",
    "    fcn = np.cos(particles[t-1]) ** 2\n",
    "    mean = C * fcn\n",
    "    sigma = np.sqrt(C * Q * C + R)\n",
    "    measurement_proposal = scipy.stats.norm(mean, sigma)\n",
    "\n",
    "    # compute weights (nu)\n",
    "    log_weights_unnorm = measurement_proposal.logpdf(y_data[t])\n",
    "    log_weights_max = np.max(log_weights_unnorm)\n",
    "    weights_unnorm = np.exp(log_weights_unnorm - log_weights_max) + log_weights_max\n",
    "    weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "    ancestor_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "\n",
    "    # PROPAGATE\n",
    "    # state\n",
    "    fcn = np.cos(particles[t-1][ancestor_indices]) ** 2\n",
    "    mean = fcn + K * (y_data[t] - C * fcn)\n",
    "    proposal_dist = scipy.stats.norm(mean, np.sqrt(Sigma))\n",
    "    particles[t] = proposal_dist.rvs()\n",
    "    # measurement (optional)\n",
    "    measurement_dist = scipy.stats.norm(C * np.mean(particles[t]), np.sqrt(R))\n",
    "    mean_observation[t] = measurement_dist.mean()\n",
    "    std_observation[t] = measurement_dist.std()\n",
    "\n",
    "    # mean_marginal_filtering[t] = np.mean(proposal_dist.mean())  # particles incorporate y_data from same time step (hence filtering)\n",
    "    mean_marginal_filtering[t] = np.mean(proposal_dist.rvs())  # particles incorporate y_data from same time step (hence filtering)\n",
    "    std_marginal_filtering[t] = np.mean(proposal_dist.std())\n",
    "\n",
    "    fcn = np.cos(particles[t-1]) ** 2  # no resampling here\n",
    "    prediction_dist = scipy.stats.norm(fcn, np.sqrt(Q))  # prediction formed by ignoring y_data (not available)\n",
    "    mean_state_prediction[t] = np.mean(prediction_dist.mean())\n",
    "    std_state_prediction[t] = np.mean(prediction_dist.std())\n",
    "\n",
    "    # loglikelihood += np.log(np.sum(weights_unnorm)) - np.log(N)\n",
    "\n",
    "weights = np.array(weights[:-1])\n",
    "particles = np.array(particles[-1:] + particles[1:-1])  # move initial particle to index 0  #  np.array(particles[:-1])\n",
    "mean_marginal_filtering = np.array(mean_marginal_filtering)\n",
    "std_marginal_filtering = np.array(std_marginal_filtering)\n",
    "mean_state_prediction = np.array(mean_state_prediction)\n",
    "std_state_prediction = np.array(std_state_prediction)\n",
    "mean_observation = np.array(mean_observation)\n",
    "std_observation = np.array(std_observation)\n",
    "loglikelihood = np.array(loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_marginal_filtering)\n",
    "plt.plot(std_marginal_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_marginal_filtering)\n",
    "plt.plot(mean_state_prediction);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data)\n",
    "plt.plot(mean_observation)\n",
    "plt.plot(std_observation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data[100:200])\n",
    "plt.plot(mean_observation[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data[100:102])\n",
    "plt.plot(mean_observation[100:102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of APF and BPF estimator variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Particle Filter\n",
    "\n",
    "bpf_means_of_estimates = []\n",
    "\n",
    "for repeat in range(10):\n",
    "\n",
    "    initial_particle_dist = scipy.stats.norm(1, 1)\n",
    "    weights = [np.array([1/N] * N)] + [None] * T\n",
    "    particles = [initial_particle_dist.rvs(N)] + [None] * T  # draw initial particles\n",
    "    mean_observation = [None] * T\n",
    "    prediction = [None] * T\n",
    "    marginal_filtering = [None] * T\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # RESAMPLE\n",
    "        ancestor_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = np.cos(particles[t][ancestor_indices]) ** 2\n",
    "        proposal_dist = scipy.stats.norm(fcn, np.sqrt(Q))\n",
    "        particles[t+1] = proposal_dist.rvs()\n",
    "\n",
    "        # measurement\n",
    "        measurement_dist = scipy.stats.norm(2 * particles[t+1], np.sqrt(R))\n",
    "        # mean observation\n",
    "        mean_observation[t] = scipy.stats.norm(2 * np.mean(particles[t+1]), np.sqrt(R)).rvs()\n",
    "\n",
    "        # WEIGHT\n",
    "        weights[t+1] = measurement_dist.logpdf(y_data[t])\n",
    "        weights[t+1] = np.exp(weights[t+1] - np.max(weights[t+1]))\n",
    "        weights[t+1] = weights[t+1] / np.sum(weights[t+1])\n",
    "\n",
    "        prediction[t] = np.mean(particles[t])\n",
    "        marginal_filtering[t] = np.sum(weights[t] * particles[t])\n",
    "\n",
    "    weights = np.array(weights[:-1])\n",
    "    particles = np.array(particles[:-1])\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    prediction = np.array(prediction)\n",
    "    marginal_filtering = np.array(marginal_filtering)\n",
    "\n",
    "    bpf_means_of_estimates.append(marginal_filtering)\n",
    "    \n",
    "bpf_means_of_estimates = np.array(bpf_means_of_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Adapted Particle Filter\n",
    "\n",
    "N = 500\n",
    "\n",
    "apf_means_of_estimates = []\n",
    "\n",
    "for repeat in range(10):\n",
    "    loglikelihood = 0\n",
    "\n",
    "    initial_particle_dist = scipy.stats.norm(1, 1)  # the actual best initial distribution\n",
    "    weights = [np.array([1/N] * N)] + [None] * T  # these are nu weights\n",
    "    particles = [None] * T + [initial_particle_dist.rvs(N)]  # draw initial particles - put at index -1\n",
    "    mean_observation = [None] * T  # p(y_t|x_t)\n",
    "    std_observation = [None] * T\n",
    "    # mean_observation_prediction = [None] * T  # p(y_t|x_t-1)\n",
    "    # std_observation_prediction = [None] * T\n",
    "    mean_state_prediction = [None] * T  # p(x_t|x_t-1)\n",
    "    std_state_prediction = [None] * T\n",
    "    mean_marginal_filtering = [None] * T  # p(x_t|x_t-1, y_t)\n",
    "    std_marginal_filtering = [None] * T\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # RESAMPLE\n",
    "        # measurement\n",
    "        fcn = np.cos(particles[t-1]) ** 2\n",
    "        mean = C * fcn\n",
    "        sigma = np.sqrt(C * Q * C + R)\n",
    "        measurement_proposal = scipy.stats.norm(mean, sigma)\n",
    "\n",
    "        # compute weights (nu)\n",
    "        log_weights_unnorm = measurement_proposal.logpdf(y_data[t])\n",
    "        log_weights_max = np.max(log_weights_unnorm)\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - log_weights_max) + log_weights_max\n",
    "        weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        ancestor_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = np.cos(particles[t-1][ancestor_indices]) ** 2\n",
    "        mean = fcn + K * (y_data[t] - C * fcn)\n",
    "        proposal_dist = scipy.stats.norm(mean, np.sqrt(Sigma))\n",
    "        particles[t] = proposal_dist.rvs()\n",
    "        # measurement (optional)\n",
    "        measurement_dist = scipy.stats.norm(C * np.mean(particles[t]), np.sqrt(R))\n",
    "        mean_observation[t] = measurement_dist.mean()\n",
    "        std_observation[t] = measurement_dist.std()\n",
    "\n",
    "        # mean_marginal_filtering[t] = np.mean(proposal_dist.mean())  # particles incorporate y_data from same time step (hence filtering)\n",
    "        mean_marginal_filtering[t] = np.mean(proposal_dist.rvs())  # particles incorporate y_data from same time step (hence filtering)\n",
    "        std_marginal_filtering[t] = np.mean(proposal_dist.std())\n",
    "\n",
    "        fcn = np.cos(particles[t-1]) ** 2  # no resampling here\n",
    "        prediction_dist = scipy.stats.norm(fcn, np.sqrt(Q))  # prediction formed by ignoring y_data (not available)\n",
    "        mean_state_prediction[t] = np.mean(prediction_dist.mean())\n",
    "        std_state_prediction[t] = np.mean(prediction_dist.std())\n",
    "\n",
    "        # loglikelihood += np.log(np.sum(weights_unnorm)) - np.log(N)\n",
    "\n",
    "    weights = np.array(weights[:-1])\n",
    "    particles = np.array(particles[-1:] + particles[1:-1])  # move initial particle to index 0  #  np.array(particles[:-1])\n",
    "    mean_marginal_filtering = np.array(mean_marginal_filtering)\n",
    "    std_marginal_filtering = np.array(std_marginal_filtering)\n",
    "    mean_state_prediction = np.array(mean_state_prediction)\n",
    "    std_state_prediction = np.array(std_state_prediction)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    std_observation = np.array(std_observation)\n",
    "    loglikelihood = np.array(loglikelihood)\n",
    "\n",
    "    apf_means_of_estimates.append(mean_marginal_filtering)\n",
    "    \n",
    "apf_means_of_estimates = np.array(apf_means_of_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_estimate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.std(bpf_means_of_estimates, axis=0), label=\"BPF variance\")\n",
    "plt.plot(np.std(apf_means_of_estimates, axis=0), label=\"APF variance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Estimator variance\")\n",
    "plt.yscale(\"log\")\n",
    "print(np.mean(np.std(bpf_means_of_estimates, axis=0)))\n",
    "print(np.mean(np.std(apf_means_of_estimates, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1047fe110e6526ec8270bc6abda2b9b08acd2c82835ba9522086c3ef7bec77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('seq-mc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
