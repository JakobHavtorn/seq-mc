{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Importance sampling theory [5p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sample(lambd, n_samples: list):\n",
    "    proposal = scipy.stats.norm(0, 1/lambd)\n",
    "    target = scipy.stats.norm(0, 1)\n",
    "    all_weights = []\n",
    "    all_samples = []\n",
    "    for N in n_samples:\n",
    "        samples = proposal.rvs(size=N)\n",
    "        weights = np.exp(target.logpdf(samples) - proposal.logpdf(samples))\n",
    "        all_samples.append(samples)\n",
    "        all_weights.append(weights)\n",
    "    return all_samples, all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = list(range(10, 10000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1.5\n",
    "all_samples, all_weights = importance_sample(lambd, n_samples)\n",
    "normalizing_constants = [np.mean(weights) for weights in all_weights]\n",
    "plt.plot(n_samples, normalizing_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 2.1\n",
    "all_samples, all_weights = importance_sample(lambd, n_samples)\n",
    "normalizing_constants = [np.mean(weights) for weights in all_weights]\n",
    "plt.plot(n_samples, normalizing_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variances = []\n",
    "# for lambd in tqdm([0.1, 1, 1.9, 2.1, 3]):\n",
    "#     normalizing_constants = []\n",
    "#     for r in tqdm(range(10)):\n",
    "#         all_samples, all_weights = importance_sample(lambd, n_samples)\n",
    "#         normalizing_constants.append([np.mean(weights) for weights in all_weights])  # [[mean(weights) for N in n_samples]]\n",
    "    \n",
    "#     normalizing_constants = np.array(normalizing_constants)\n",
    "#     variances.append(np.var(normalizing_constants, axis=0))\n",
    "\n",
    "# variances = np.array(variances)\n",
    "# variances.shape\n",
    "\n",
    "# plt.plot(n_samples, variances.T)\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "n1 = scipy.stats.norm(0, 1).logpdf(x)\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 0.3).logpdf(x), label=\"$\\sigma=0.3$\")\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 0.4).logpdf(x), label=\"$\\sigma=0.4$\")\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 0.5).logpdf(x), label=\"$\\sigma=0.5$\")\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 0.75).logpdf(x), label=\"$\\sigma=0.75$\")\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 1).logpdf(x), label=\"$\\sigma=1$\")\n",
    "plt.plot(x, n1 - scipy.stats.norm(0, 1.5).logpdf(x), label=\"$\\sigma=1.5$\")\n",
    "plt.ylabel(\"$\\log w = \\log \\mathcal{N}(x|0,1) - \\log\\mathcal{N}(0,\\sigma^2)$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/importance_sampling_sigma.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Particle filter for a linear Gaussian state-space model [16p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "A = 0.9  # state transition matrix\n",
    "Q = 0.5  # state variance\n",
    "C = 1.3  # observation matrix\n",
    "R = 0.1  # observation variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Simulate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_x(x):\n",
    "    return A * x + scipy.stats.norm(0, np.sqrt(Q)).rvs()\n",
    "\n",
    "def step_y(x):\n",
    "    return C * x + scipy.stats.norm(0, np.sqrt(R)).rvs(x.shape[0])\n",
    "\n",
    "def simulate(initial_x, step_x_fcn, step_y_fcn, n_timesteps):\n",
    "    xs = [initial_x] + [None] * n_timesteps\n",
    "    for t in range(n_timesteps):\n",
    "        xs[t+1] = step_x_fcn(xs[t])\n",
    "    \n",
    "    xs = np.array(xs[1:])\n",
    "    ys = step_y_fcn(xs)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "T = 2000\n",
    "initial_x = np.random.normal(0, 1)\n",
    "x_data, y_data = simulate(initial_x, step_x, step_y, 2000)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data, label=\"$y_t$\")\n",
    "plt.plot(x_data, label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data[100:200], label=\"$y_t$\")\n",
    "plt.plot(x_data[100:200], label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Kalman Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_and_var(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return (average, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "P0 = 1  # initial state variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "initial_x = 0  # scipy.stats.norm(0, P0).rvs()\n",
    "initial_Pt_filtering = P0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(initial_x, initial_Pt_filtering, y_data, A, C, Q, R):\n",
    "    T = len(y_data)\n",
    "    xs = [None] * T + [initial_x]\n",
    "    Pts_filtering = [None] * T + [initial_Pt_filtering]\n",
    "    for t in range(T):\n",
    "        Pt_predictive = A * Pts_filtering[t-1] * A + Q\n",
    "\n",
    "        Kt = Pt_predictive * C / (C * Pt_predictive * C + R)\n",
    "\n",
    "        # state update\n",
    "        xs[t] = A * xs[t-1] + Kt * (y_data[t] - C * A * xs[t-1])\n",
    "\n",
    "        # variance update\n",
    "        Pts_filtering[t] = Pt_predictive - Kt * C * Pt_predictive\n",
    "\n",
    "    xs = np.array(xs[:-1])\n",
    "    Pts_filtering = np.array(Pts_filtering[:-1])\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        particles=xs,\n",
    "        variances=Pts_filtering\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_out = kalman_filter(initial_x, initial_Pt_filtering, y_data, A, C, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_out.particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(200,250,1), kalman_out.particles[200:250], label=\"Kalman Filter $\\hat{x}_t$\")\n",
    "plt.plot(range(200,250,1), x_data[200:250], label=\"Data $x_t$\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/kalman_filter_particles_zoom.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kalman_out.particles, label=\"Kalman Filter $\\hat{x}_t$\")\n",
    "plt.plot(x_data, label=\"Data $x_t$\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/kalman_filter_particles.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((kalman_out.particles - x_data), label=\"Mean absolute error of Kalman Filter\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\hat{x}_t-x_t|$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/kalman_filter_mean_absolute_error.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kalman_out.variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Bootstrap Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pf(initial_particles, y_data, A, C, Q, R, verbose=True, seed=0):\n",
    "    \"\"\"Bootstrap Particle Filter\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = len(initial_particles)\n",
    "    T = len(y_data)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles\")\n",
    "    weights = [None] * T + [np.array([1/N] * N)]\n",
    "    particles = [None] * T + [initial_particles]\n",
    "    mean_filtering = [None] * T\n",
    "    var_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "\n",
    "    iterator = tqdm(range(T)) if verbose else range(T)\n",
    "    for t in iterator:\n",
    "        # RESAMPLE\n",
    "        ancestor_indices[t] = np.random.choice(range(N), p=weights[t-1], replace=True, size=N)\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = A * particles[t-1][ancestor_indices[t]]\n",
    "        proposal_dist = scipy.stats.norm(fcn, np.sqrt(Q))\n",
    "        particles[t] = proposal_dist.rvs()\n",
    "        # measurement\n",
    "        fcn = C * particles[t]\n",
    "        measurement_dist = scipy.stats.norm(fcn, np.sqrt(R))\n",
    "\n",
    "        # WEIGHT\n",
    "        log_weights_unnorm = measurement_dist.logpdf(y_data[t])\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "        weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        mean_filtering[t], var_filtering[t] = weighted_mean_and_var(particles[t], weights[t])\n",
    "\n",
    "    weights = np.array(weights[:-1])\n",
    "    particles = np.array(particles[:-1])\n",
    "    mean_filtering = np.array(mean_filtering)\n",
    "    var_filtering = np.array(var_filtering)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "    \n",
    "    output = SimpleNamespace(\n",
    "        weights=weights,\n",
    "        particles=particles,\n",
    "        mean_filtering=mean_filtering,\n",
    "        var_filtering=var_filtering,\n",
    "        ancestor_indices=ancestor_indices,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "bpf_out = bootstrap_pf(initial_particles, y_data, A, C, Q, R)\n",
    "\n",
    "bpf_out.mean_filtering.shape, bpf_out.ancestor_indices.shape, kalman_out.particles.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.mean_filtering, label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data, label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.mean_filtering[200:250], label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data[200:250], label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((bpf_out.mean_filtering - x_data), label=\"$|\\hat{x}_t - x_t$|\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.var_filtering, label=\"Var\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.mean_filtering, label=\"BPF\")\n",
    "plt.plot(kalman_out.particles, label=\"Kalman\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$\\mathbb{E}[p(x_t|y_{1:t})]$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/bpf_kalman_filter_means.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.var_filtering, label=\"BPF\")\n",
    "plt.plot(kalman_out.variances, label=\"Kalman\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$Var[p(x_t|y_{1:t})]$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/bpf_kalman_filter_variances.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(bpf_out.mean_filtering - kalman_out.particles))\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\hat{x}_{t,BPF}-\\hat{x}_{t,Kalman}|$\")\n",
    "plt.savefig(\"./figures/bpf_filter_mean_absolute_error_to_kalman_filter.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error: \", np.mean(np.abs(bpf_out.mean_filtering - kalman_out.particles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(bpf_out.var_filtering - kalman_out.variances))\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\widehat{\\sigma^2}_{t,BPF}-\\widehat{\\sigma^2}_{t,Kalman}|$\")\n",
    "plt.savefig(\"./figures/bpf_filter_var_absolute_error_to_kalman_filter.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error: \", np.mean(np.abs(bpf_out.var_filtering - kalman_out.variances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "\n",
    "Ns = [10, 50, 100, 2000, 5000]\n",
    "\n",
    "bpf_all_mean_filtering = []\n",
    "bpf_all_var_filtering = []\n",
    "\n",
    "for N in Ns:\n",
    "    initial_particles = initial_particle_dist.rvs(N)\n",
    "    out_bpf = bootstrap_pf(initial_particles, y_data, A, C, Q, R, verbose=1)\n",
    "    bpf_all_mean_filtering.append(out_bpf.mean_filtering)\n",
    "    bpf_all_var_filtering.append(out_bpf.var_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_differences_of_mean = [np.mean(np.abs(kalman_out.particles - np.array(mean_filtering))) for mean_filtering in bpf_all_mean_filtering]\n",
    "avg_absolute_differences_of_var = [np.mean(np.abs(kalman_out.variances - np.array(var_filtering))) for var_filtering in bpf_all_var_filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Fully Adapted Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_op_exp(array, op=np.mean, axis=-1):\n",
    "    \"\"\"Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
    "\n",
    "    :param array: Tensor to compute LSE over\n",
    "    :param axis: dimension to perform operation over\n",
    "    :param op: reductive operation to be applied, e.g. np.sum or np.mean\n",
    "    :return: LSE\n",
    "    \"\"\"\n",
    "    maximum = np.max(array, axis=axis)\n",
    "    return np.log(op(np.exp(array - maximum), axis=axis) + 1e-8) + maximum\n",
    "\n",
    "\n",
    "def systematic_resampling(w, n_strata=None):\n",
    "    n_strata = len(w) if n_strata is None else n_strata\n",
    "    u = (np.arange(n_strata) + np.random.rand())/n_strata\n",
    "    bins = np.cumsum(w)\n",
    "    return np.digitize(u, bins)\n",
    "\n",
    "\n",
    "def multinmomial_resampling(w, N=None):\n",
    "    N = len(w) if N is None else N\n",
    "    return np.random.choice(range(N), p=w, replace=True, size=N)\n",
    "\n",
    "\n",
    "def compute_ess(w):\n",
    "    return 1 / np.sum(w ** 2)\n",
    "\n",
    "\n",
    "def fully_adapted_pf(initial_particles, y_data, A, C, Q, R, resampling=\"multinomial\", ess_trigger=None, verbose=0, seed=0):\n",
    "    \"\"\"Fully Adapted Particle Filter\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    N = len(initial_particles)\n",
    "    T = len(y_data)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles\")\n",
    "\n",
    "    nu_weights = [None] * T  # these are nu weights\n",
    "    particles = [None] * T + [initial_particles]  # draw initial particles - put at index -1\n",
    "    mean_observation = [None] * T  # p(y_t|x_t)\n",
    "    var_observation = [None] * T\n",
    "    mean_state_prediction = [None] * T  # p(x_t|x_t-1)\n",
    "    var_state_prediction = [None] * T\n",
    "    mean_filtering = [None] * T  # p(x_t|x_t-1, y_t)\n",
    "    var_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "    effective_sample_size = [None] * T\n",
    "    loglikelihood = 0\n",
    "    \n",
    "    if ess_trigger is None:\n",
    "        ess_trigger = N\n",
    "        N_ess = 0\n",
    "        do_adaptive_resample = False\n",
    "    else:\n",
    "        do_adaptive_resample = True\n",
    "    \n",
    "    importance_weights = np.array([1/N] * N)\n",
    "    \n",
    "    if resampling == \"multinomial\":\n",
    "        resample = multinmomial_resampling\n",
    "    elif resampling == \"systematic\":\n",
    "        resample = systematic_resampling\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resampling method: {resampling}\")\n",
    "    \n",
    "    K = Q * C / (C * Q * C + R)\n",
    "    state_proposal_stddev = np.sqrt((1 - K * C) * Q)\n",
    "    obs_proposal_stddev = np.sqrt(C * Q * C + R)\n",
    "    \n",
    "    iterator = tqdm(range(T)) if verbose else range(T)\n",
    "    for t in iterator:\n",
    "        # WEIGHT\n",
    "        # measurement\n",
    "        fcn_weight = A * particles[t-1]\n",
    "        mean = C * fcn_weight\n",
    "        measurement_proposal_dist = scipy.stats.norm(mean, obs_proposal_stddev)\n",
    "\n",
    "        # compute weights (nu)\n",
    "        log_nu_weights_unnorm = measurement_proposal_dist.logpdf(y_data[t])\n",
    "        nu_weights_unnorm = np.exp(log_nu_weights_unnorm - np.max(log_nu_weights_unnorm))\n",
    "        nu_weights[t] = nu_weights_unnorm / np.sum(nu_weights_unnorm)\n",
    "\n",
    "        # RESAMPLE\n",
    "        if do_adaptive_resample:\n",
    "            N_ess = compute_ess(nu_weights[t])\n",
    "            effective_sample_size[t] = N_ess\n",
    "        if N_ess < ess_trigger:\n",
    "            a_indices = resample(nu_weights[t])\n",
    "        else:\n",
    "            a_indices = np.arange(N)\n",
    "        ancestor_indices[t] = a_indices\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn_prop = fcn_weight[a_indices]\n",
    "        mean = fcn_prop + K * (y_data[t] - C * fcn_prop)\n",
    "        state_proposal_dist = scipy.stats.norm(mean, state_proposal_stddev)\n",
    "        particles[t] = state_proposal_dist.rvs()\n",
    "        # measurement (optional)\n",
    "        measurement_dist = scipy.stats.norm(C * np.mean(particles[t]), np.sqrt(R))\n",
    "        mean_observation[t] = measurement_dist.mean()\n",
    "        var_observation[t] = measurement_dist.var()\n",
    "\n",
    "        mean_filtering[t], var_filtering[t] = weighted_mean_and_var(particles[t], importance_weights)\n",
    "\n",
    "        state_prediction_dist = scipy.stats.norm(fcn_weight, np.sqrt(Q))  # prediction formed by ignoring y_data (not available)\n",
    "        mean_state_prediction[t] = np.mean(state_prediction_dist.mean())\n",
    "        var_state_prediction[t] = np.mean(state_prediction_dist.var())\n",
    "\n",
    "        # likelihood\n",
    "        log_obs = measurement_dist.logpdf(y_data[t])\n",
    "        log_state_pred = state_prediction_dist.logpdf(particles[t])\n",
    "        log_state_prop = state_proposal_dist.logpdf(particles[t])\n",
    "        loglikelihood_term = log_obs + log_state_pred - log_state_prop - np.log(nu_weights[t][a_indices]) - np.log(N)\n",
    "        loglikelihood += log_op_exp(loglikelihood_term, np.mean)\n",
    "\n",
    "    nu_weights = np.array(nu_weights)\n",
    "    particles = np.array(particles[:-1])\n",
    "    mean_filtering = np.array(mean_filtering)\n",
    "    var_filtering = np.array(var_filtering)\n",
    "    mean_state_prediction = np.array(mean_state_prediction)\n",
    "    var_state_prediction = np.array(var_state_prediction)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    var_observation = np.array(var_observation)\n",
    "    loglikelihood = np.array(loglikelihood)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "    effective_sample_size = np.array(effective_sample_size)\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        nu_weights=nu_weights,\n",
    "        particles=particles,\n",
    "        mean_filtering=mean_filtering,\n",
    "        var_filtering=var_filtering,\n",
    "        mean_state_prediction=mean_state_prediction,\n",
    "        var_state_prediction=var_state_prediction,\n",
    "        mean_observation=mean_observation,\n",
    "        var_observation=var_observation,\n",
    "        loglikelihood=loglikelihood,\n",
    "        ancestor_indices=ancestor_indices,\n",
    "        effective_sample_size=effective_sample_size,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "fapf_out = fully_adapted_pf(initial_particles, y_data, A, C, Q, R, verbose=1, seed=0)\n",
    "\n",
    "fapf_out.mean_filtering.shape, fapf_out.ancestor_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Bootstrap Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.mean_filtering, label=\"BPF filtering\")\n",
    "plt.plot(fapf_out.mean_filtering, label=\"FAPF filtering\")\n",
    "plt.plot(kalman_out.particles, label=\"Kalman filtering\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_out.var_filtering, label=\"BPF variance\")\n",
    "plt.plot(fapf_out.var_filtering, label=\"FAPF variance\")\n",
    "plt.plot(kalman_out.variances, label=\"Kalman variance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(bpf_out.mean_filtering - kalman_out.particles), label=\"BPF\")\n",
    "plt.plot(np.abs(fapf_out.mean_filtering - kalman_out.particles), label=\"AFPF\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\hat{x}_{t}-\\hat{x}_{t,Kalman}|$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/fapf_mean_absolute_error_to_bpf.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error (BPF): \", np.mean(np.abs(bpf_out.mean_filtering - kalman_out.particles)))\n",
    "print(\"Mean Absolute Error (FAPF): \", np.mean(np.abs(fapf_out.mean_filtering - kalman_out.particles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(bpf_out.var_filtering - kalman_out.variances), label=\"BPF\")\n",
    "plt.plot(np.abs(fapf_out.var_filtering - kalman_out.variances), label=\"AFPF\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\widehat{\\sigma^2}_{t,BPF}-\\widehat{\\sigma^2}_{t,Kalman}|$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/fapf_var_absolute_error_to_bpf.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error (BPF): \", np.mean(np.abs(bpf_out.var_filtering - kalman_out.variances)))\n",
    "print(\"Mean Absolute Error (FAPF): \", np.mean(np.abs(fapf_out.var_filtering - kalman_out.variances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fapf_out.mean_filtering, label=\"FAPF\")\n",
    "plt.plot(kalman_out.particles, label=\"Kalman\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$\\mathbb{E}[p(x_t|y_{1:t})]$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/fapf_kalman_filter_means.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fapf_out.var_filtering, label=\"FAPF\")\n",
    "plt.plot(kalman_out.variances, label=\"Kalman\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$Var[p(x_t|y_{1:t})]$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./figures/fapf_kalman_filter_variances.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.abs(fapf_out.mean_filtering - kalman_out.particles))\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\hat{x}_{t,FAPF}-\\hat{x}_{t,Kalman}|$\")\n",
    "plt.savefig(\"./figures/fapf_filter_mean_absolute_error_to_kalman_filter.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error: \", np.mean(np.abs(fapf_out.mean_filtering - kalman_out.particles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(fapf_out.var_filtering - kalman_out.variances))\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$|\\widehat{\\sigma^2}_{t,FAPF}-\\widehat{\\sigma^2}_{t,Kalman}|$\")\n",
    "plt.savefig(\"./figures/fapf_filter_var_absolute_error_to_kalman_filter.pdf\", bbox_inches=\"tight\")\n",
    "print(\"Mean Absolute Error: \", np.mean(np.abs(fapf_out.var_filtering - kalman_out.variances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "\n",
    "Ns = [10, 50, 100, 2000, 5000]\n",
    "\n",
    "fapf_all_mean_filtering = []\n",
    "fapf_all_var_filtering = []\n",
    "\n",
    "for N in Ns:\n",
    "    initial_particles = initial_particle_dist.rvs(N)\n",
    "    fapf_out = fully_adapted_pf(initial_particles, y_data, A, C, Q, R, verbose=1, seed=0)\n",
    "    fapf_all_mean_filtering.append(fapf_out.mean_filtering)\n",
    "    fapf_all_var_filtering.append(fapf_out.var_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_differences_of_mean = [np.mean(np.abs(kalman_out.particles - np.array(mean_filtering))) for mean_filtering in fapf_all_mean_filtering]\n",
    "avg_absolute_differences_of_var = [np.mean(np.abs(kalman_out.variances - np.array(var_filtering))) for var_filtering in fapf_all_var_filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Genealogy of Fully Adapted Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack_genealogy(list_index, list_sample):\n",
    "    \"\"\"Requires initial particle to be at list_sample[-1] and len(list_sample) = len(list_index) + 1\"\"\"\n",
    "    aux_list_index = copy.deepcopy(list_index)\n",
    "    genealogy = [list_sample[-2].reshape(1, -1)]  # list_sample[-2] is the last particle\n",
    "\n",
    "    T = len(list_index)\n",
    "    for t in range(T - 1, -1, -1):  # [4, 3, 2, 1, 0]\n",
    "        genealogy.insert(0, list_sample[t-1][aux_list_index[t]].reshape(1, -1))\n",
    "        aux_list_index[t-1] = aux_list_index[t-1][aux_list_index[t]]\n",
    "\n",
    "    genealogy = np.concatenate(genealogy, axis=0)  # (x_0, x_1, x_2, ..., x_T)\n",
    "    return genealogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_genealogy(genealogy, particles, ancestor_indices, t1=None, t2=None, reference_trajectory=None, sampled_trajectory=None, verbose=0, figsize=(20, 10), alpha=0.3):\n",
    "    \"\"\"Requires initial particle to be at particles[-1] and len(particles) = len(ancestor_indices) + 1\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    if t1 is None:\n",
    "        t1 = 0\n",
    "    if t2 is None:\n",
    "        t2 = len(ancestor_indices)  # T == len(y_data)\n",
    "\n",
    "    assert t2 <= len(ancestor_indices)\n",
    "\n",
    "    T = t2 - t1\n",
    "    \n",
    "    # [t1, t1+1, ..., t2-2, t2-1]\n",
    "    iterator = tqdm(range(t1, t2)) if verbose else range(t1, t2)\n",
    "    for i, t in enumerate(iterator):\n",
    "        p = np.array([particles[t-1][ancestor_indices[t]], particles[t]])\n",
    "        ax.plot([i, i+1], p, marker='o', color='silver', alpha=alpha)\n",
    "\n",
    "    # plot first particles (initial) without ancestral resampling (the above makes some particles at t1-1 not appear)\n",
    "    ax.plot([0, 0], [particles[t1-1,:-1], particles[t1-1,:-1]], marker='o', color='silver', alpha=alpha)\n",
    "    ax.plot([0, 0], [particles[t1-1,-1:], particles[t1-1,-1:]], marker='o', color='silver', alpha=alpha, label=\"Particles\")\n",
    "    if verbose:\n",
    "        print(\"Plotted particles and connecting ancestral lines\")\n",
    "\n",
    "    # [t1, t1+1, ..., t2-1, t2]\n",
    "    ax.plot(genealogy[t1:t2+1 ,:-1], marker='o', color='tab:red')\n",
    "    ax.plot(genealogy[t1:t2+1 ,-1:], marker='o', color='tab:red', label=\"Genealogy\")\n",
    "    if verbose:\n",
    "        print(\"Plotted genealogy\")\n",
    "    \n",
    "    if reference_trajectory is not None:\n",
    "        # put initial at front\n",
    "        reference_trajectory_plot = np.concatenate([reference_trajectory[-1:], reference_trajectory[:-1]])\n",
    "        ax.plot(reference_trajectory_plot[t1:t2], color=\"tab:blue\", label=\"Reference trajectory\")\n",
    "        if verbose:\n",
    "            print(\"Plotted reference tracjectory\")\n",
    "\n",
    "    if sampled_trajectory is not None:\n",
    "        # put initial at front\n",
    "        sampled_trajectory_plot = np.concatenate([sampled_trajectory[-1:], sampled_trajectory[:-1]])\n",
    "        ax.plot(sampled_trajectory_plot[t1:t2], color=\"tab:green\", label=\"Sampled trajectory\")\n",
    "        if verbose:\n",
    "            print(\"Plotted sampled tracjectory\")\n",
    "\n",
    "\n",
    "    ticks = np.linspace(0, T, 11).astype(int).tolist()\n",
    "    labels = np.linspace(t1, t2, 11).astype(int).tolist()\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "t = T\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "fapf_out = fully_adapted_pf(initial_particles, y_data[:t], A, C, Q, R, verbose=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append initial particle to list of particles\n",
    "if fapf_out.particles.shape[0] == T:\n",
    "    fapf_out.particles = np.concatenate([fapf_out.particles, initial_particles[np.newaxis]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fapf_out.particles.shape, fapf_out.ancestor_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fapf_out.genealogy = backtrack_genealogy(fapf_out.ancestor_indices, fapf_out.particles)\n",
    "genealogy = fapf_out.genealogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genealogy.shape, fapf_out.particles.shape, fapf_out.ancestor_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-10\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_out.particles, fapf_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-30\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_out.particles, fapf_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-100\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_out.particles, fapf_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "t2 = 30\n",
    "fig, ax = plot_genealogy(genealogy, fapf_out.particles, fapf_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Genealogy of Fully Adapted Particle Filtering with Systematic Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "t = T\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "fapf_sys_out = fully_adapted_pf(initial_particles, y_data[:t], A, C, Q, R, resampling=\"systematic\", verbose=1, seed=0)\n",
    "\n",
    "# Append initial particle to list of particles\n",
    "fapf_sys_out.particles = np.concatenate([fapf_sys_out.particles, initial_particles[np.newaxis]], axis=0)\n",
    "\n",
    "genealogy = backtrack_genealogy(fapf_sys_out.ancestor_indices, fapf_sys_out.particles)\n",
    "\n",
    "fapf_sys_out.genealogy = genealogy\n",
    "\n",
    "genealogy.shape, fapf_sys_out.particles.shape, fapf_sys_out.ancestor_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-10\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_out.particles, fapf_sys_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-100\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_out.particles, fapf_sys_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "t2 = 30\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_out.particles, fapf_sys_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Genealogy of Fully Adapted Particle Filtering with Systematic and Adaptive Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "t = T\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "fapf_sys_adap_out = fully_adapted_pf(initial_particles, y_data[:t], A, C, Q, R, resampling=\"systematic\", ess_trigger=N//2, verbose=1, seed=0)\n",
    "\n",
    "# Append initial particle to list of particles\n",
    "fapf_sys_adap_out.particles = np.concatenate([fapf_sys_adap_out.particles, initial_particles[np.newaxis]], axis=0)\n",
    "\n",
    "genealogy = backtrack_genealogy(fapf_sys_adap_out.ancestor_indices, fapf_sys_adap_out.particles)\n",
    "\n",
    "fapf_sys_adap_out.genealogy = genealogy\n",
    "\n",
    "genealogy.shape, fapf_sys_adap_out.particles.shape, fapf_sys_adap_out.ancestor_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fapf_sys_adap_out.effective_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0\n",
    "t2 = 30\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_adap_out.particles, fapf_sys_adap_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_adap_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-10\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_adap_out.particles, fapf_sys_adap_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_adap_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = T-100\n",
    "t2 = T\n",
    "fig, ax = plot_genealogy(genealogy, fapf_sys_adap_out.particles, fapf_sys_adap_out.ancestor_indices, t1=t1, t2=t2, verbose=True)\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$x_t$\")\n",
    "fig.savefig(f\"./figures/afpf_sys_adap_genealogy_{N}_particles_timesteps_{t1}_to_{t2}.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fapf_sys_adap_out.effective_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fapf_sys_adap_out.effective_sample_size / N, label=\"$N_{eff}\\,/\\,N$\")\n",
    "plt.plot([0, T], [0.5, 0.5], label=\"Threshold\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$N_{eff}\\,/\\,N$\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./figures/afpf_sys_adap_Neff_ratio_{N}_particles.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the number of unique paths in genealogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_paths(genealogy, axis=1):\n",
    "    b = np.sort(genealogy, axis=axis)\n",
    "    n_paths = (b[:, 1:] != b[:, :-1]).sum(axis=axis) + 1\n",
    "    return n_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(compute_n_paths(fapf_out.genealogy), label=\"Multinomial resamplnig\")\n",
    "plt.plot(compute_n_paths(fapf_sys_out.genealogy), label=\"Systematic resampling\")\n",
    "plt.plot(compute_n_paths(fapf_sys_adap_out.genealogy), label=\"Systematic resampling with adaptive resampling\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Number of paths in genealogy\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(f\"./figures/afpf_sys_adap_number_of_active_paths_{N}_particles.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Parameter estimation in the stochastic volatility model [11p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack_genealogy(list_index, list_sample):\n",
    "    \"\"\"Requires initial particle to be at list_sample[-1] and len(list_sample) = len(list_index) + 1\"\"\"\n",
    "    aux_list_index = copy.deepcopy(list_index)\n",
    "    genealogy = [list_sample[-2].reshape(1, -1)]  # list_sample[-2] is the last particle\n",
    "\n",
    "    T = len(list_index)\n",
    "    for t in range(T - 1, -1, -1):  # [4, 3, 2, 1, 0]\n",
    "        genealogy.insert(0, list_sample[t-1][aux_list_index[t]].reshape(1, -1))\n",
    "        aux_list_index[t-1] = aux_list_index[t-1][aux_list_index[t]]\n",
    "\n",
    "    genealogy = np.concatenate(genealogy, axis=0)  # (x_0, x_1, x_2, ..., x_T)\n",
    "    return genealogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Grid search for phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pf_stochastic_volatility(initial_particles, phi, sigma, beta, verbose=True, seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    N = len(initial_particles)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles\")\n",
    "\n",
    "    loglikelihood = 0\n",
    "    weights = [np.array([1/N] * N)] + [None] * T\n",
    "    particles = [initial_particles] + [None] * T  # draw initial particles\n",
    "    mean_observation = [None] * T\n",
    "    prediction = [None] * T\n",
    "    marginal_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "\n",
    "    for t in range(T):\n",
    "        # RESAMPLE\n",
    "        a_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "        ancestor_indices[t] = a_indices\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        proposal_dist = scipy.stats.norm(phi * particles[t][a_indices], sigma)\n",
    "        particles[t+1] = proposal_dist.rvs()\n",
    "\n",
    "        # measurement\n",
    "        measurement_dist = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(particles[t+1])))\n",
    "        # mean observation\n",
    "        mean_observation[t] = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(np.mean(particles[t+1])))).rvs()\n",
    "\n",
    "        # WEIGHT\n",
    "        log_weights_unnorm = measurement_dist.logpdf(observation_data[t])\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "        weights[t+1] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        prediction[t] = np.mean(particles[t])\n",
    "        marginal_filtering[t] = np.sum(weights[t] * particles[t])\n",
    "\n",
    "        loglikelihood += np.log(np.sum(weights_unnorm)) - np.log(N) + np.max(log_weights_unnorm)\n",
    "\n",
    "    particles = np.array(particles[:-1])  # remove initial state\n",
    "    marginal_filtering = np.array(marginal_filtering)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "    loglikelihood = np.array(loglikelihood)\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        particles=particles,\n",
    "        marginal_filtering=marginal_filtering,\n",
    "        mean_observation=mean_observation,\n",
    "        ancestor_indices=ancestor_indices,\n",
    "        loglikelihood=loglikelihood,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_data = pd.read_csv(\"./seOMXlogreturns2012to2014.csv\")\n",
    "T = observation_data.shape[0]\n",
    "observation_data = observation_data.to_numpy()[:, 0]\n",
    "observation_data.shape, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phis = np.linspace(0.1, 1, 10)  # 0.98\n",
    "phis = np.linspace(0.95, 1, 11)  # 0.98\n",
    "sigma = 0.16\n",
    "beta = 0.70\n",
    "\n",
    "N = 500\n",
    "initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "\n",
    "phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Particle Filter parameter estimation via grid\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "loglikelihood = []\n",
    "bpf_all_output = []\n",
    "\n",
    "for phi in tqdm(phis):\n",
    "    loglikelihood_ = []\n",
    "    output_ = []\n",
    "\n",
    "    for repeat in range(10):\n",
    "\n",
    "        initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "        output = bootstrap_pf_stochastic_volatility(initial_particles, phi, sigma, beta, verbose=False, seed=None)\n",
    "        \n",
    "        loglikelihood_.append(output.loglikelihood)\n",
    "        output_.append(output)\n",
    "\n",
    "    bpf_all_output.append(output_)\n",
    "    loglikelihood.append(loglikelihood_)\n",
    "\n",
    "loglikelihood = np.array(loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(loglikelihood)\n",
    "best_loglikelihood = loglikelihood.reshape(-1)[best_idx]\n",
    "best_output = bpf_all_output[best_idx]\n",
    "best_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(loglikelihood, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.boxplot(loglikelihood.T)\n",
    "ax.set_xticklabels(phis.round(3))\n",
    "ax.set_xlabel(\"$\\phi$\")\n",
    "ax.set_ylabel(\"log-likelihood\")\n",
    "plt.savefig(f\"./figures/phis_loglikelihood_bpf_{min(phis)}_{max(phis)}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Particle Metropolis Hastings for parameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_correction(current, proposal, proposal_dist):\n",
    "    proposal_relative = current - proposal + proposal_dist.mean()\n",
    "    current_relative = proposal - current + proposal_dist.mean()\n",
    "    proposal_prob = proposal_dist.logpdf(proposal_relative).sum()  # Sum over dim of parameters\n",
    "    current_prob = proposal_dist.logpdf(current_relative).sum()  # Sum over dim of parameters\n",
    "    return proposal_prob - current_prob\n",
    "\n",
    "\n",
    "def particle_metropolis_hastings(n_steps, initial_param, param_random_walk_proposal, param_prior_logpdf, initial_particle_dist, n_particles, phi, burn_in=100, verbose=0, seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if burn_in >= n_steps:\n",
    "        raise ValueError(f\"We need burn_in < n_steps\")\n",
    "    if burn_in == 0:\n",
    "        burn_in = 1  # discard initial reference trajectory\n",
    "\n",
    "    current_param = initial_param\n",
    "    initial_particles = initial_particle_dist.rvs(n_particles)\n",
    "    output = bootstrap_pf_stochastic_volatility(initial_particles, phi, sigma=current_param[0], beta=current_param[1], verbose=verbose>1, seed=None)\n",
    "    current_loglikelihood = output.loglikelihood\n",
    "\n",
    "    params = []\n",
    "    loglikelihoods = []\n",
    "    for m in range(n_steps):\n",
    "        proposed_param = current_param + param_random_walk_proposal.rvs() - param_random_walk_proposal.mean()\n",
    "\n",
    "        if proposed_param[0] < 0 or proposed_param[1] < 0:\n",
    "            # if the proposed parameters are out of domain, we perform the Metropolis rejection already here.\n",
    "            # if verbose > 1:\n",
    "            print(f\"Rejected run {m} due to domain error in the proposed parameters\")\n",
    "\n",
    "            params.append(current_param)\n",
    "            loglikelihoods.append(current_loglikelihood)\n",
    "            continue\n",
    "\n",
    "        initial_particles = initial_particle_dist.rvs(n_particles)\n",
    "        output = bootstrap_pf_stochastic_volatility(initial_particles, phi, sigma=proposed_param[0], beta=proposed_param[1], verbose=verbose>1, seed=None)\n",
    "        proposed_loglikelihood = output.loglikelihood\n",
    "\n",
    "        correction = mh_correction(current_param, proposed_param, param_random_walk_proposal)\n",
    "\n",
    "        proposed_param_logprob = param_prior_logpdf(proposed_param ** 2)  # square since prior is over sigma**2 and beta**2\n",
    "        current_param_logprob = param_prior_logpdf(current_param ** 2)\n",
    "        \n",
    "        acceptance = proposed_param_logprob - current_param_logprob + proposed_loglikelihood - current_loglikelihood + correction\n",
    "        event = np.log(np.random.uniform(0, 1))\n",
    "        if acceptance > event:\n",
    "            current_param = proposed_param\n",
    "            current_loglikelihood = proposed_loglikelihood\n",
    "\n",
    "        params.append(current_param)\n",
    "        loglikelihoods.append(current_loglikelihood)\n",
    "        \n",
    "        if verbose:\n",
    "            l = len(loglikelihoods)\n",
    "            accept_rate = round(len(np.unique(loglikelihoods[l//2:])) / len(loglikelihoods[l//2:]), 3) * 100\n",
    "            print(f\"{m}/{n_steps} | acc_prob={np.exp(acceptance)*100:4.1f}, acc_rate={accept_rate:4.1f}, current_param={list(current_param)}\")\n",
    "        \n",
    "    return np.array(params[burn_in:]), np.array(loglikelihoods[burn_in:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_prior_logpdf(params):\n",
    "    d = scipy.stats.invgamma(a=0.01, scale=0.01)\n",
    "    return d.logpdf(params[0]) + d.logpdf(params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_prior_pdf(params):\n",
    "    # d = scipy.stats.uniform(0, 1)\n",
    "    d = scipy.stats.invgamma(a=0.01, scale=0.01)\n",
    "    return np.exp(d.logpdf(params[0]) + d.logpdf(params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rw_proposal = scipy.stats.norm([0, 0], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rw_proposal.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate parameters (infer posterior p(sigma,beta|y_data)) using Particle Metropolis Hastings\n",
    "# M=30 sigma=0.05 ar=0.02\n",
    "# M=30 sigma=0.01 ar=0.52\n",
    "param_rw_proposal = scipy.stats.norm([0, 0], 0.1)\n",
    "initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "initial_param = np.array([0.5, 0.5])\n",
    "phi = 0.985\n",
    "N = 500  # Number of APF particles\n",
    "M = 1000  # Number of PMH runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loglikelihoods = particle_metropolis_hastings(M, initial_param, param_rw_proposal, param_prior_logpdf, initial_particle_dist, N, phi, verbose=1, seed=0)\n",
    "len(params), len(loglikelihoods), len(np.unique(loglikelihoods)), round(len(np.unique(loglikelihoods[M//2:])) / len(loglikelihoods[M//2:]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loglikelihoods, bins=50, density=True)\n",
    "plt.xlabel(\"Log-Likelihood\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.savefig(f\"./figures/sigma_beta_loglikelihoods_{N}_particles_{M}_pmh_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[np.argmax(loglikelihoods)], np.argmax(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(params[:,0], bins=50, density=True, label=\"Estimated posterior\")\n",
    "plt.plot([0.16, 0.16], [0, plt.gca().get_ylim()[1]], 'r', label='True value')\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./figures/marginal_posterior_sigma_{N}_particles_{M}_pmh_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(params[:,1], bins=50, density=True, label=\"Estimated posterior\")\n",
    "plt.plot([0.70, 0.70], [0, plt.gca().get_ylim()[1]], 'r', label='True value')\n",
    "plt.xlabel(\"beta\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./figures/marginal_posterior_beta_{N}_particles_{M}_pmh_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(params[:,0], loglikelihoods);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(params[:,1], loglikelihoods);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Particle Gibbs for parameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pf_gibbs_stochastic_volatility(N, initial_particle_dist, reference_trajectory, phi, sigma, beta, observation_data, verbose=True, seed=None):\n",
    "    \"\"\"BPF as a Particle Gibs Kernel.\n",
    "\n",
    "    Args:\n",
    "        initial_particles: (N,)\n",
    "        reference_trajectory: (T,) with initial value at index -1\n",
    "        phi ([type]): parameter\n",
    "        sigma ([type]): parameter\n",
    "        beta ([type]): parameter\n",
    "        verbose (bool, optional): Print statuses. Defaults to True.\n",
    "        seed (int, optional): Seed. Defaults to 0.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles\")\n",
    "\n",
    "    initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "    # deterministically set reference trajectory\n",
    "    if reference_trajectory is not None:\n",
    "        initial_particles[-1] = reference_trajectory[-1]  # Condition on the reference trajectory\n",
    "\n",
    "    loglikelihood = 0\n",
    "    weights = [None] * T + [np.array([1/N] * N)]\n",
    "    particles = [None] * T + [initial_particles]  # draw initial particles\n",
    "    mean_observation = [None] * T\n",
    "    prediction = [None] * T\n",
    "    marginal_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "\n",
    "    for t in range(T):\n",
    "        # RESAMPLE\n",
    "        a_indices = np.random.choice(range(N), p=weights[t-1], replace=True, size=N)\n",
    "        ancestor_indices[t] = a_indices\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        proposal_dist = scipy.stats.norm(phi * particles[t-1][a_indices], sigma)\n",
    "        particles[t] = proposal_dist.rvs()\n",
    "        \n",
    "        # deterministically set reference trajectory\n",
    "        if reference_trajectory is not None:\n",
    "            particles[t][-1] = reference_trajectory[t]  # Condition on the reference trajectory\n",
    "            ancestor_indices[t][-1] = N - 1  # Update according to reference trajectory\n",
    "\n",
    "        # measurement\n",
    "        measurement_dist = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(particles[t])))\n",
    "        # mean observation\n",
    "        mean_observation[t] = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(np.mean(particles[t])))).rvs()\n",
    "\n",
    "        # WEIGHT\n",
    "        log_weights_unnorm = measurement_dist.logpdf(observation_data[t])\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "        weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        prediction[t] = np.mean(particles[t])\n",
    "        marginal_filtering[t] = np.sum(weights[t] * particles[t])\n",
    "\n",
    "        loglikelihood += np.log(np.sum(weights_unnorm)) - np.log(N) + np.max(log_weights_unnorm)\n",
    "\n",
    "    genealogy = backtrack_genealogy(ancestor_indices, particles)\n",
    "    j = np.random.choice(range(N), p=weights[T-1], replace=False, size=1)\n",
    "    reference_trajectory = genealogy[:, j].reshape(-1)  # (T+1, N) -> (T+1,)\n",
    "    reference_trajectory = np.concatenate([reference_trajectory[1:], reference_trajectory[:1]])  # put initial at end\n",
    "\n",
    "    particles = np.array(particles[:-1])  # remove initial state\n",
    "    marginal_filtering = np.array(marginal_filtering)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    loglikelihood = np.array(loglikelihood)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        particles=particles,\n",
    "        marginal_filtering=marginal_filtering,\n",
    "        mean_observation=mean_observation,\n",
    "        loglikelihood=loglikelihood,\n",
    "        ancestor_indices=ancestor_indices,\n",
    "        reference_trajectory=reference_trajectory\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_gibbs_sampler(M, init_params, markov_kernel, sample_parameter_conditional_dist, observation_data, burn_in=100, verbose=False, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if burn_in >= M:\n",
    "        raise ValueError(f\"We need burn_in < M\")\n",
    "    if burn_in == 0:\n",
    "        burn_in = 1  # discard initial reference trajectory\n",
    "\n",
    "    parameters = [init_params] + [None] * M\n",
    "    reference_trajectories = [None] + [None] * M\n",
    "    loglikelihoods = [None] + [None] * M\n",
    "    \n",
    "    # get initial reference trajectory from initial parameters\n",
    "    kernel_out = markov_kernel(\n",
    "        reference_trajectory=None,\n",
    "        sigma=parameters[0][0],\n",
    "        beta=parameters[0][1],\n",
    "    )\n",
    "    reference_trajectories[0] = kernel_out.reference_trajectory\n",
    "    \n",
    "    loglikelihoods[0] = kernel_out.loglikelihood\n",
    "    \n",
    "    # iterator = tqdm(range(1, M)) if verbose else range(1, M)\n",
    "    for m in range(1, M + 1):\n",
    "        parameters[m] = sample_parameter_conditional_dist(reference_trajectories[m-1], observation_data)\n",
    "        \n",
    "        kernel_out = markov_kernel(\n",
    "            reference_trajectory=reference_trajectories[m-1],\n",
    "            sigma=parameters[m][0],\n",
    "            beta=parameters[m][1],\n",
    "        )\n",
    "        reference_trajectories[m] = kernel_out.reference_trajectory\n",
    "        \n",
    "        loglikelihoods[m] = kernel_out.loglikelihood\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{m:3d}/{M:3d} | {parameters[m]} | {loglikelihoods[m]} | {np.min(reference_trajectories[m])}, {np.max(reference_trajectories[m])}, {np.mean(reference_trajectories[m])}\")\n",
    "\n",
    "    reference_trajectories = reference_trajectories[burn_in:]\n",
    "    parameters = parameters[burn_in:]\n",
    "    loglikelihoods = loglikelihoods[burn_in:]\n",
    "\n",
    "    parameters = np.stack([np.array(p) for p in parameters])\n",
    "    reference_trajectories = np.stack(reference_trajectories)\n",
    "    loglikelihoods = np.stack(loglikelihoods)\n",
    "\n",
    "    return reference_trajectories, parameters, loglikelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_parameter_conditional_dist(states, observation_data):\n",
    "    a = 0.01\n",
    "    b = 0.01\n",
    "\n",
    "    a_new = a + T/2\n",
    "    b_sigma = b + 1/2 * np.sum( (states[1:] - phi * states[:-1]) ** 2 )\n",
    "    b_beta = b + 1/2 * np.sum( np.exp(-states[1:]) * observation_data ** 2 )\n",
    "\n",
    "    sigma_cond = scipy.stats.invgamma(a=a_new, scale=b_sigma)\n",
    "    beta_cond = scipy.stats.invgamma(a=a_new, scale=b_beta)\n",
    "    \n",
    "    return np.sqrt(sigma_cond.rvs()), np.sqrt(beta_cond.rvs())\n",
    "\n",
    "\n",
    "N = 5000\n",
    "initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "phi = 0.985\n",
    "\n",
    "markov_kernel = partial(\n",
    "    bootstrap_pf_gibbs_stochastic_volatility,\n",
    "    N=N,\n",
    "    initial_particle_dist=initial_particle_dist,\n",
    "    phi=phi,\n",
    "    observation_data=observation_data,\n",
    "    verbose=False,\n",
    "    seed=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 150\n",
    "initial_parameters = [0.5, 0.5]\n",
    "\n",
    "states, parameters, loglikelihoods = particle_gibbs_sampler(\n",
    "    M,\n",
    "    initial_parameters,\n",
    "    markov_kernel,\n",
    "    sample_parameter_conditional_dist,\n",
    "    burn_in=100 if M > 100 else 0,\n",
    "    observation_data=observation_data,\n",
    "    verbose=True,\n",
    "    seed=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[np.argmax(loglikelihoods)], np.argmax(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loglikelihoods, bins=50, density=True)\n",
    "plt.xlabel(\"Log-Likelihood\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.savefig(f\"./figures/sigma_beta_loglikelihoods_{N}_particles_{M}_pg_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters[:, 0], bins=50, density=True, label=\"Estimated posterior\")\n",
    "plt.plot([0.16, 0.16], [0, plt.gca().get_ylim()[1]], 'r', label='True value')\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./figures/marginal_posterior_sigma_{N}_particles_{M}_pg_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters[:, 1], bins=50, density=True, label=\"Estimated posterior\")\n",
    "plt.plot([0.70, 0.70], [0, plt.gca().get_ylim()[1]], 'r', label='True value')\n",
    "plt.xlabel(\"beta\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./figures/marginal_posterior_beta_{N}_particles_{M}_pg_iterations.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(parameters[:, 0], loglikelihoods);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(parameters[:, 1], loglikelihoods);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parameter_conditional_dist(initial_reference_trajectory, observation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_output.particles = np.concatenate([best_output.particles, scipy.stats.norm(0,1).rvs(N)[np.newaxis]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genealogy = backtrack_genealogy(best_output.ancestor_indices, best_output.particles)\n",
    "\n",
    "j = 0\n",
    "reference_trajectory = genealogy[:, j].reshape(-1)  # (T+1, N) -> (T+1,)\n",
    "reference_trajectory = np.concatenate([reference_trajectory[1:], reference_trajectory[:1]])  # put initial at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parameter_conditional_dist(reference_trajectory, observation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(reference_trajectory), np.max(reference_trajectory), np.mean(reference_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: SMC sampler [8p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1047fe110e6526ec8270bc6abda2b9b08acd2c82835ba9522086c3ef7bec77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('seq-mc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
