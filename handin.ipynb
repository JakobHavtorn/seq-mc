{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sample(lambd, n_samples: list):\n",
    "    proposal = scipy.stats.norm(0, 1/lambd)\n",
    "    target = scipy.stats.norm(0, 1)\n",
    "    all_weights = []\n",
    "    all_samples = []\n",
    "    for N in n_samples:\n",
    "        samples = proposal.rvs(size=N)\n",
    "        weights = np.exp(target.logpdf(samples) - proposal.logpdf(samples))\n",
    "        all_samples.append(samples)\n",
    "        all_weights.append(weights)\n",
    "    return all_samples, all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = list(range(10, 10000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1.5\n",
    "all_samples, all_weights = importance_sample(lambd, n_samples)\n",
    "normalizing_constants = [np.mean(weights) for weights in all_weights]\n",
    "plt.plot(n_samples, normalizing_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 2.5\n",
    "all_samples, all_weights = importance_sample(lambd, n_samples)\n",
    "normalizing_constants = [np.mean(weights) for weights in all_weights]\n",
    "plt.plot(n_samples, normalizing_constants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "A = 0.9  # state transition matrix\n",
    "Q = 0.5  # state variance\n",
    "C = 1.3  # observation matrix\n",
    "R = 0.1  # observation variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Simulate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_x(x):\n",
    "    return A * x + scipy.stats.norm(0, np.sqrt(Q)).rvs() # np.random.normal(scale=np.sqrt(Q))\n",
    "\n",
    "def step_y(x):\n",
    "    return C * x + scipy.stats.norm(0, np.sqrt(R)).rvs(x.shape[0]) # np.random.normal(scale=np.sqrt(R))\n",
    "\n",
    "def simulate(initial_x, step_x_fcn, step_y_fcn, n_timesteps):\n",
    "    xs = [initial_x] + [None] * n_timesteps\n",
    "    for t in range(n_timesteps):\n",
    "        xs[t+1] = step_x_fcn(xs[t])\n",
    "    \n",
    "    xs = np.array(xs[1:])\n",
    "    ys = step_y_fcn(xs)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "T = 2000\n",
    "initial_x = np.random.normal(0, 1)\n",
    "x_data, y_data = simulate(initial_x, step_x, step_y, 2000)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data, label=\"$y_t$\")\n",
    "plt.plot(x_data, label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_data[100:200], label=\"$y_t$\")\n",
    "plt.plot(x_data[100:200], label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Kalman Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_and_var(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return (average, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "P0 = 1  # initial state variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "initial_x = 0  # scipy.stats.norm(0, P0).rvs()\n",
    "initial_Pt_filtering = P0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter(initial_x, initial_Pt_filtering, T, A, C, Q, R):\n",
    "    xs = [initial_x] + [None] * T\n",
    "    Pts_filtering = [initial_Pt_filtering] + [None] * T\n",
    "    for t in range(T):\n",
    "        Pt_predictive = A * Pts_filtering[t] * A + Q\n",
    "\n",
    "        Kt = Pt_predictive * C / (C * Pt_predictive * C + R)\n",
    "\n",
    "        # state update\n",
    "        xs[t+1] = A * xs[t] + Kt * (y_data[t] - C * A * xs[t])\n",
    "\n",
    "        # variance update\n",
    "        Pts_filtering[t+1] = Pt_predictive - Kt * C * Pt_predictive\n",
    "\n",
    "    xs = np.array(xs[1:])\n",
    "    Pts_filtering = np.array(Pts_filtering[1:])\n",
    "    return xs, Pts_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_particles, kalman_variances = kalman_filter(initial_x, initial_Pt_filtering, T, A, C, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kalman_particles[200:250], label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data[200:250], label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kalman_particles, label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data, label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((kalman_particles - x_data), label=\"$|\\hat{x}_t - x_t$|\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kalman_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Bootstrap Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Particle Filter\n",
    "\n",
    "def bootstrap_pf(initial_particles, A, C, Q, R, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    N = len(initial_particles)\n",
    "    print(f\"Running with {N} particles\")\n",
    "    weights = [np.array([1/N] * N)] + [None] * T\n",
    "    particles = [initial_particles] + [None] * T\n",
    "    mean_filtering = [None] * T\n",
    "    var_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # RESAMPLE\n",
    "        a_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "        ancestor_indices[t] = a_indices\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = A * particles[t][a_indices]\n",
    "        proposal_dist = scipy.stats.norm(fcn, np.sqrt(Q))\n",
    "        particles[t+1] = proposal_dist.rvs()\n",
    "        # measurement\n",
    "        fcn = C * particles[t+1]\n",
    "        measurement_dist = scipy.stats.norm(fcn, np.sqrt(R))\n",
    "\n",
    "        # WEIGHT\n",
    "        log_weights_unnorm = measurement_dist.logpdf(y_data[t])\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "        weights[t+1] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        mean_filtering[t], var_filtering[t] = weighted_mean_and_var(particles[t+1], weights[t+1])\n",
    "\n",
    "    weights = np.array(weights[1:])\n",
    "    particles = np.array(particles[1:])\n",
    "    mean_filtering = np.array(mean_filtering)\n",
    "    var_filtering = np.array(var_filtering)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "    return particles, weights, mean_filtering, var_filtering, ancestor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "initial_particles = initial_particle_dist.rvs(N)\n",
    "\n",
    "bpf_particles, bpf_weights, bpf_mean_filtering, bpf_var_filtering, bpf_ancestor_indices = bootstrap_pf(initial_particles, A, C, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_mean_filtering, label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data, label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_mean_filtering[200:250], label=\"$\\hat{x}_t$\")\n",
    "plt.plot(x_data[200:250], label=\"$x_t$\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((bpf_mean_filtering - x_data), label=\"$|\\hat{x}_t - x_t$|\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_var_filtering, label=\"Var\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to the Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_mean_filtering, label=\"BPF particles\")\n",
    "plt.plot(kalman_particles, label=\"Kalman particles\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bpf_var_filtering, label=\"BPF variance\")\n",
    "plt.plot(kalman_variances, label=\"Kalman variance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_particle_dist = scipy.stats.norm(0, np.sqrt(P0))\n",
    "\n",
    "Ns = [10, 50, 100, 2000, 5000, 100000]\n",
    "\n",
    "all_mean_filtering = []\n",
    "all_var_filtering = []\n",
    "\n",
    "for N in Ns:\n",
    "    initial_particles = initial_particle_dist.rvs(N)\n",
    "    particles, weights, mean_filtering, var_filtering, ancestor_indices = bootstrap_pf(initial_particles, A, C, Q, R)\n",
    "    all_mean_filtering.append(mean_filtering)\n",
    "    all_var_filtering.append(var_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_absolute_differences_of_mean = [np.mean(np.abs(kalman_particles - np.array(mean_filtering))) for mean_filtering in all_mean_filtering]\n",
    "avg_absolute_differences_of_var = [np.mean(np.abs(kalman_variances - np.array(var_filtering))) for var_filtering in all_var_filtering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns, avg_absolute_differences_of_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Fully Adapted Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_particle_dist = scipy.stats.norm(1, 1)  # the actual best initial distribution\n",
    "\n",
    "# Fully Adapted Particle Filter\n",
    "\n",
    "def fully_adapted_pf(initial_particles, A, C, Q, R, seed=0):\n",
    "    np.random.seed(0)\n",
    "    N = len(initial_particles)\n",
    "    weights = [np.array([1/N] * N)] + [None] * T  # these are nu weights\n",
    "    particles = [None] * T + [initial_particles]  # draw initial particles - put at index -1\n",
    "    mean_observation = [None] * T  # p(y_t|x_t)\n",
    "    std_observation = [None] * T\n",
    "    mean_state_prediction = [None] * T  # p(x_t|x_t-1)\n",
    "    std_state_prediction = [None] * T\n",
    "    mean_marginal_filtering = [None] * T  # p(x_t|x_t-1, y_t)\n",
    "    std_marginal_filtering = [None] * T\n",
    "    loglikelihood = 0\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # WEIGHT\n",
    "        # measurement\n",
    "        fcn = np.cos(particles[t-1]) ** 2\n",
    "        mean = C * fcn\n",
    "        sigma = np.sqrt(C * Q * C + R)\n",
    "        measurement_proposal = scipy.stats.norm(mean, sigma)\n",
    "\n",
    "        # compute weights (nu)\n",
    "        log_weights_unnorm = measurement_proposal.logpdf(y_data[t])\n",
    "        log_weights_max = np.max(log_weights_unnorm)\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - log_weights_max) + log_weights_max\n",
    "        weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        # RESAMPLE\n",
    "        ancestor_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = np.cos(particles[t-1][ancestor_indices]) ** 2\n",
    "        mean = fcn + K * (y_data[t] - C * fcn)\n",
    "        proposal_dist = scipy.stats.norm(mean, np.sqrt(Sigma))\n",
    "        particles[t] = proposal_dist.rvs()\n",
    "        # measurement (optional)\n",
    "        measurement_dist = scipy.stats.norm(C * np.mean(particles[t]), np.sqrt(R))\n",
    "        mean_observation[t] = measurement_dist.mean()\n",
    "        std_observation[t] = measurement_dist.std()\n",
    "\n",
    "        # mean_marginal_filtering[t] = np.mean(proposal_dist.mean())  # particles incorporate y_data from same time step (hence filtering)\n",
    "        mean_marginal_filtering[t] = np.mean(proposal_dist.rvs())  # particles incorporate y_data from same time step (hence filtering)\n",
    "        std_marginal_filtering[t] = np.mean(proposal_dist.std())\n",
    "\n",
    "        fcn = np.cos(particles[t-1]) ** 2  # no resampling here\n",
    "        prediction_dist = scipy.stats.norm(fcn, np.sqrt(Q))  # prediction formed by ignoring y_data (not available)\n",
    "        mean_state_prediction[t] = np.mean(prediction_dist.mean())\n",
    "        std_state_prediction[t] = np.mean(prediction_dist.std())\n",
    "\n",
    "        # loglikelihood += np.log(np.sum(weights_unnorm)) - np.log(N)\n",
    "\n",
    "    weights = np.array(weights[:-1])\n",
    "    particles = np.array(particles[-1:] + particles[1:-1])  # move initial particle to index 0  #  np.array(particles[:-1])\n",
    "    mean_marginal_filtering = np.array(mean_marginal_filtering)\n",
    "    std_marginal_filtering = np.array(std_marginal_filtering)\n",
    "    mean_state_prediction = np.array(mean_state_prediction)\n",
    "    std_state_prediction = np.array(std_state_prediction)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    std_observation = np.array(std_observation)\n",
    "    loglikelihood = np.array(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Genealogy of Fully Adapted Particle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack_genealogy(list_index, list_sample):\n",
    "    aux_list_index = copy.deepcopy(list_index)\n",
    "    genealogy = [list_sample[-1].reshape(1,-1)]\n",
    "    \n",
    "    for k in range(len(list_index)-1, 0, -1):\n",
    "        index_previous = aux_list_index[k]\n",
    "        aux_list_index[k-1] = aux_list_index[k-1][index_previous]\n",
    "        genealogy.insert(0, list_sample[k-1][index_previous].reshape(1,-1))\n",
    "  \n",
    "    genealogy = np.concatenate(genealogy,axis =0)\n",
    "    return genealogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.plot(list(range(T)), genealogy, marker='o', color='red')  #, linestyle='--')\n",
    "\n",
    "for t in range(T - 1):\n",
    "    p = np.array([particles[t][ancestor_indices[t+1]], particles[t+1]])\n",
    "    ax.plot([t, t+1], p, marker='o', color='grey', alpha=0.5);  #, linestyle='--')\n",
    "\n",
    "ax.plot([0, 0], [particles[0], particles[0]], marker='o', color='grey', alpha=0.5);  #, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Genealogy of Fully Adapted Particle Filtering with Systematic Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_resampling(w, x, n_strata=None):\n",
    "    n_strata = len(w) if n_strata is None else n_strata\n",
    "    u = (np.arange(n_strata) + np.random.rand())/n_strata\n",
    "    bins = np.cumsum(w)\n",
    "    return x[np.digitize(u,bins)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Genealogy of Fully Adapted Particle Filtering with Systematic and Adaptive Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1047fe110e6526ec8270bc6abda2b9b08acd2c82835ba9522086c3ef7bec77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('seq-mc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
