{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Examples of Sampling Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from .mcmc import visualize_sampling_1d, visualize_sampling_2d\n",
    "from .mcmc import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inverse transform sampling\n",
    "\n",
    "We can use the inverse cumulative density function of a target distribution to convert samples from the uniform distribution into samples from the target distribution. This is a very general technique for producing samples from any distribution with an inverse CDF.\n",
    "\n",
    "Since the CDF has range $(0, 1)$, we effectively sample uniformly in the range, and convert to the corresponding value in the domain. Let the CDF of some target distribution $p(z)$ be denoted by $f(\\cdot)$. Then\n",
    "\n",
    "1. Draw $u\\sim \\mathcal{U}(0, 1)$\n",
    "\n",
    "2. Transform $z = f^{-1}(u)$ to obtain a sample $z\\sim p(z)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_dist = D.Normal(torch.Tensor([0]), torch.Tensor([1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples = inverse_transform_sampling(target_dist.icdf, n_samples=1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_1d(samples, target_dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rejection sampling\n",
    "https://en.wikipedia.org/wiki/Rejection_sampling#Adaptive_rejection_sampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#target_dist = D.Uniform(0, 1)\n",
    "target_dist = D.Normal(1.0, 0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#proposal_dist = D.Independent(D.Normal(torch.tensor([0.5]), torch.tensor([0.5])), 1)\n",
    "proposal_dist = D.Normal(0.5, 0.5)\n",
    "proposal_dist.batch_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame({'Target distribution': target_dist.sample((10000,)),\n",
    "                   'Proposal distribution': proposal_dist.sample((10000,)).flatten()})\n",
    "ax = sns.displot(df, kde=True, stat='density')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if hasattr(target_dist.support, 'lower_bound') and hasattr(target_dist.support, 'upper_bound'):\n",
    "    val = torch.linspace(target_dist.support.lower_bound, target_dist.support.upper_bound, 500)\n",
    "else:\n",
    "    val = torch.linspace(-3, 3, 500)\n",
    "M = (target_dist.log_prob(val) - proposal_dist.log_prob(val)).max().exp()\n",
    "print(M)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples, estimands = rejection_sampling(n_samples=1000, proposal_dist=proposal_dist,\n",
    "                                        target_dist_log_prob=target_dist.log_prob, M=M)\n",
    "estimands"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "visualize_sampling_1d(samples, target_dist)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Markov Chain Monte Carlo Samplers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO \n",
    "- Add evaluation metrics (R_hat, correlation between chains etc.) as in BDA 3\n",
    "- Maybe make classes for the sampling methods and a base-class with the common evaluation metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#class MCMCSampler():\n",
    "#    def __init__(self):"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gibbs sampling\n",
    "\n",
    "Gibbs sampling is also called \"alternating conditional sampling\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metropolis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metropolis sampling samples from a target distribution $p(x)$, which is assumed difficult to sample from but easy to evaluate, by instead drawing samples from a (symmetric) proposal distribution $q(x)$, which is easy to sample from. Proposed samples are then accepted or rejected based on the probability ratio of the previous accepted sample to the new sample as evaluated under the target distribution (or a function that is proportional to it. $f(x) \\propto p(x)$)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $f(x)$ be a function that is proportional to the desired probability distribution $p(x)$ (a.k.a. a target distribution).\n",
    "\n",
    "- Initialization:\n",
    "    - Choose an arbitrary point $x_t$ to be the first sample\n",
    "    - Choose an arbitrary probability density $g(x\\mid y)$ (sometimes written $Q(x\\mid y)$) that suggests a candidate for the next sample value $x$, given the previous sample value $y$. For Metropolis sampling, $g$ is assumed to be symmetric; in other words, it must satisfy $g(x\\mid y) = g(y\\mid x)$. A usual choice is to let $g(x\\mid y)$ be a Gaussian distribution centered at $y$, so that points closer to $y$ are more likely to be visited next, making the sequence of samples into a random walk.  The function $g$ is referred to as the **proposal density** or **jumping distribution**.\n",
    "- For each iteration **t**:\n",
    "    - **Generate** a candidate $x'$ for the next sample by picking from the distribution $g(x'\\mid x_t)$.\n",
    "    - **Calculate** the **acceptance ratio** $\\alpha = f(x')/f(x_t)$, which will be used to decide whether to accept or reject the candidate. Because **f** is proportional to the density of **P**, we have that $\\alpha = f(x')/f(x_t) = P(x')/P(x_t)$.\n",
    "    - **Accept or reject**: \n",
    "        - Generate a uniform random number $u \\in [0, 1]$.\n",
    "        - If $u \\le \\alpha$, then **accept** the candidate by setting $x_{t+1} = x'$,\n",
    "        - If $u > \\alpha$, then **reject** the candidate and set $x_{t+1} = x_t$ instead.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_dist = D.MultivariateNormal(loc=torch.Tensor([0,0]), covariance_matrix=4 * torch.Tensor([[1, 0], [0, 1]]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_point = torch.Tensor([0, 10])\n",
    "n_samples = 1000\n",
    "samples, accept_reject = metropolis(initial_point, n_samples, proposal_dist, target_dist.log_prob)\n",
    "torch.tensor(accept_reject).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_2d(samples, target_dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metropolis-Hastings\n",
    "https://en.wikipedia.org/wiki/Metropolisâ€“Hastings_algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Metropolis algorithm requires a symmetric proposal distribution. The Metropolis-Hastings algorithm relaxes this requirement by computing the likelihood ratio of the proposals drawn under the proposal distribution and correcting the likelihood ratio of the proposals under the target distribution. This effectively corrects for the effects of an asymmetric proposal distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mixture of zero mean small variance gaussian and zero mean high variance gaussian\n",
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[0, 0]]), torch.Tensor([[0.5, 0.5], [5, 5]])), 1)\n",
    "proposal_dist = D.MixtureSameFamily(mix, comp)\n",
    "\n",
    "#proposal_dist = D.Independent(D.LogNormal(torch.tensor([1.0, 1.0]), torch.tensor([1.0, 1.0])), 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_point = torch.tensor([0.1, 5.0])\n",
    "n_samples = 3000\n",
    "samples, accept_reject = metropolis_hastings(initial_point, n_samples, proposal_dist, target_dist.log_prob)\n",
    "torch.tensor(accept_reject).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_2d(samples[n_samples // 2:], target_dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batched Metropolis Hastings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_chains = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_dist = D.Independent(D.Normal(torch.tensor([0.0, 0.0]), torch.tensor([2.5, 3.5])), 1)\n",
    "proposal_dist.batch_shape, proposal_dist.event_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_point = torch.tensor([2, 2.5]) + 1.5 * torch.randn(size=(n_chains, 2,)) \n",
    "n_samples = 1000\n",
    "samples, estimands = metropolis_hastings_batched(initial_point, n_samples, proposal_dist, target_dist.log_prob)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metrics, variogram, autocorrelations, T = convergence_test(estimands[0], estimands[1][:n_samples // 2])\n",
    "metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = {name: estimands[1][..., i].flatten() for i, name in enumerate(estimands[0])}\n",
    "series = pd.DataFrame(data)\n",
    "series.describe().T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "series.quantile([0.05, 0.95]).T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.quantile(estimands[1][..., 2], 0.05), np.quantile(estimands[1][..., 2], 0.95)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_2d(\n",
    "    samples.view(-1, 2)[n_samples // 2:],\n",
    "    target_dist,\n",
    "    samples[:150, :1, :]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hamiltonian Markov Chain Monte Carlo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://colab.research.google.com/drive/1YQBSfS1Nb8a9TAMsV1RjWsiErWqXLbrj#scrollTo=VY3G25_m9HBZ\n",
    "http://www.mcmchandbook.net/HandbookChapter5.pdf\n",
    "https://towardsdatascience.com/python-hamiltonian-monte-carlo-from-scratch-955dba96a42d\n",
    "https://arxiv.org/pdf/1701.02434.pdf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_dist.variance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "M = torch.Tensor([1/5, 1/10])  # Approximate inverse of covariance of target distribution\n",
    "momentum_dist = D.Independent(D.Normal(torch.Tensor([0, 0]), M), 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "momentum_dist.batch_shape, momentum_dist.event_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_point = torch.tensor([0.1, 5.0])\n",
    "n_samples = 1000\n",
    "leapfrog_steps = 10\n",
    "leapfrog_stepsize = 0.1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples, accept_reject, full_hmc_path = hamiltonian_monte_carlo(initial_point, n_samples, momentum_dist, target_dist.log_prob,\n",
    "                                                                leapfrog_steps=leapfrog_steps, leapfrog_stepsize=leapfrog_stepsize,\n",
    "                                                                return_full_path=True)\n",
    "torch.tensor(accept_reject).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_hmc_path.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_2d(samples, target_dist, connect_samples=full_hmc_path[:15])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batched HMC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_dist.stddev"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_chains = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "M = torch.Tensor([1/2, 1/3])  # Approximate inverse of covariance of target distribution\n",
    "momentum_dist = D.Independent(D.Normal(torch.Tensor([0, 0]), M), 1)\n",
    "momentum_dist.batch_shape, momentum_dist.event_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_point = torch.tensor([0.1, 5]) + 1 * torch.randn(size=(n_chains, 2))\n",
    "n_samples = 2000\n",
    "leapfrog_steps = 10\n",
    "leapfrog_stepsize = 0.1\n",
    "initial_point.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples, chains, estimand_names, estimands, full_hmc_path = hamiltonian_monte_carlo_batched(initial_point, n_samples, momentum_dist, target_dist.log_prob,\n",
    "                                                                                            leapfrog_steps=leapfrog_steps, leapfrog_stepsize=leapfrog_stepsize,\n",
    "                                                                                            return_full_path=True, use_progressbar=True)\n",
    "samples.shape, estimands[1].shape, full_hmc_path.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metrics, variogram, autocorrelations, T = convergence_test(estimand_names, estimands[n_samples // 2:])\n",
    "metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_estimates_report(estimand_names, estimands)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plot_autocorrelations(estimand_names, autocorrelations, T)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_sampling_2d(\n",
    "    chains[n_samples//2:].view(-1, 2)[:1000],\n",
    "    target_dist,\n",
    "    full_hmc_path[:10, :, 0, :]  # All leapfrog steps for 15 samples in chain 0\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assessing convergence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can we batch the gradient computation?\n",
    "Examination done before imlpementation of batched HMC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First, compute the gradient independently for two proposals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample = torch.tensor([1.0, 2.0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample.requires_grad = True\n",
    "proposal_sample.grad = None\n",
    "target_dist.log_prob(proposal_sample).backward()\n",
    "proposal_sample.requires_grad = False\n",
    "proposal_sample.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample = torch.tensor([2.0, 4.0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample.requires_grad_(True)\n",
    "proposal_sample.grad = None\n",
    "target_dist.log_prob(proposal_sample).backward()\n",
    "proposal_sample.requires_grad_(False)\n",
    "proposal_sample.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now, compute the gradient via batching by exploting summation in the `log_prob`"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mix = D.Categorical(torch.ones(2,))\n",
    "comp = D.Independent(D.Normal(torch.Tensor([[0, 0],[4, 6]]), torch.Tensor([[1, 1], [1, 2]])), 1)\n",
    "target_dist = D.MixtureSameFamily(mix, comp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample = torch.tensor([[1.0, 2.0], [2.0, 4.0]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample.requires_grad = True\n",
    "proposal_sample.grad = None\n",
    "target_dist.log_prob(proposal_sample).sum().backward()\n",
    "proposal_sample.requires_grad = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proposal_sample.grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First nonnegative element in tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = torch.randn(5, 8)\n",
    "x[:, 2] = -torch.ones(5)\n",
    "x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nonnegative = (x > 0)\n",
    "nonnegative"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cumsum = nonnegative.cumsum(axis=0)\n",
    "cumsum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "first_nonzero_bool_map = (nonnegative.cumsum(0) == 1) & nonnegative\n",
    "first_nonzero_bool_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "naive_nonzero = ((nonnegative.cumsum(0) == 1) & nonnegative).max(0).indices\n",
    "naive_nonzero"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_nonzero = cumsum[-1] == 0\n",
    "all_nonzero"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "naive_nonzero[all_nonzero] = -1\n",
    "naive_nonzero"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "first_nonzero_bool_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cumsum > 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = cumsum <= 0 \n",
    "x[~idx] = 0\n",
    "x.sum(axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "index = (torch.ones(8) * 4)\n",
    "index = index.type(torch.LongTensor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "index.dtype, index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = torch.LongTensor([1, 2, 3, 4, 0, 1, 2, 3])\n",
    "idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = torch.LongTensor([1, 2, 3, 4, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.index_select(x, 0, idx, out=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infotropy",
   "language": "python",
   "name": "infotropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}