{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "T = 50\n",
    "C = 1\n",
    "Q = 1\n",
    "R = 1\n",
    "theta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State space model\n",
    "\n",
    "def step_x(x, **kwargs):\n",
    "    return np.cos(kwargs[\"theta\"] * x)\n",
    "\n",
    "def step_y(x, C, **kwargs):\n",
    "    return C * x\n",
    "\n",
    "\n",
    "# # State space model\n",
    "\n",
    "# def step_x(x, Q, **kwargs):\n",
    "#     return kwargs[\"theta\"] * x + np.random.normal(0, np.sqrt(Q), size=x.shape)\n",
    "\n",
    "# def step_y(x, C, R, **kwargs):\n",
    "#     return C * x + np.random.normal(0, np.sqrt(R), size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "\n",
    "def simulate_ssm(T, theta):\n",
    "    x = np.zeros(T + 1)\n",
    "    y = np.zeros(T + 1)\n",
    "    x[0] = np.random.normal(0, 1)\n",
    "    y[0] = x[0] + np.random.normal(0, 1)\n",
    "    for t in range(1, T + 1):\n",
    "        x[t] = step_x(x[t-1], theta=theta) + np.random.normal(0, np.sqrt(Q), size=x[t].shape)\n",
    "        y[t] = step_y(x[t], C, theta=theta) + np.random.normal(0, np.sqrt(R), size=x[t].shape)\n",
    "    return x[1:], y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = simulate_ssm(T, theta)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data)\n",
    "plt.ylabel(\"$x_t$\")\n",
    "plt.xlabel(\"$t$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data)\n",
    "plt.plot(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Adapted Particle Filter (to estimate the likelihood z_hat = p(y_data|theta))\n",
    "\n",
    "def log_op_exp(array, op=np.mean, axis=-1):\n",
    "    \"\"\"Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
    "\n",
    "    :param array: Tensor to compute LSE over\n",
    "    :param axis: dimension to perform operation over\n",
    "    :param op: reductive operation to be applied, e.g. np.sum or np.mean\n",
    "    :return: LSE\n",
    "    \"\"\"\n",
    "    maximum = np.max(array, axis=axis)\n",
    "    return np.log(op(np.exp(array - maximum), axis=axis) + 1e-8) + maximum\n",
    "\n",
    "\n",
    "def fully_adapted_pf(initial_particles, step_x, C, Q, R, seed=0, verbose=True, **step_kwargs):\n",
    "    \"\"\"Fully adapted particle filter for a nonlinear Gaussian State Space Model.\n",
    "\n",
    "    The importance weights are uniform for this PF.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    N = len(initial_particles)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles and {step_kwargs=}\")\n",
    "    particles = [None] * T + [initial_particles]  # draw initial particles - put at index -1\n",
    "    nu_weights = [None] * T  # these are nu weights\n",
    "    mean_observation = [None] * T  # p(y_t|x_t)\n",
    "    std_observation = [None] * T\n",
    "    mean_state_prediction = [None] * T  # p(x_t|x_t-1)\n",
    "    std_state_prediction = [None] * T\n",
    "    mean_filtering = [None] * T  # p(x_t|x_t-1, y_t)\n",
    "    std_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "    loglikelihood = 0\n",
    "\n",
    "    K = Q * C / (C * Q * C + R)\n",
    "    state_proposal_stddev = np.sqrt((1 - K * C) * Q)\n",
    "    obs_proposal_stddev = np.sqrt(C * Q * C + R)\n",
    "\n",
    "    iterator = tqdm(range(T)) if verbose else range(T)\n",
    "    for t in iterator:\n",
    "        # WEIGHT\n",
    "        # measurement\n",
    "        fcn = step_x(particles[t-1], **step_kwargs)\n",
    "        obs_mean = C * fcn\n",
    "        measurement_proposal_dist = scipy.stats.norm(obs_mean, obs_proposal_stddev)  # p(y_t|x_t-1)\n",
    "\n",
    "        # compute weights (nu)\n",
    "        log_nu_weights_unnorm = measurement_proposal_dist.logpdf(y_data[t])\n",
    "        nu_weights_unnorm = np.exp(log_nu_weights_unnorm - np.max(log_nu_weights_unnorm))\n",
    "        nu_weights[t] = nu_weights_unnorm / np.sum(nu_weights_unnorm)\n",
    "\n",
    "        # RESAMPLE\n",
    "        a_indices = np.random.choice(range(N), p=nu_weights[t], replace=True, size=N)\n",
    "        ancestor_indices[t] = a_indices\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = step_x(particles[t-1][a_indices], **step_kwargs)\n",
    "        state_mean = fcn + K * (y_data[t] - C * fcn)\n",
    "        state_proposal_dist = scipy.stats.norm(state_mean, state_proposal_stddev)  # p(x_t|x_t-1^a_t,y_t)\n",
    "        particles[t] = state_proposal_dist.rvs()\n",
    "\n",
    "        # Store some statistics\n",
    "        # marginal filtering mean and variance\n",
    "        mean_filtering[t], std_filtering[t] = np.mean(particles[t]), np.std(particles[t])\n",
    "        # prediction\n",
    "        fcn = step_x(particles[t-1], **step_kwargs)  # this is done before resampling\n",
    "        state_prediction_dist = scipy.stats.norm(fcn, np.sqrt(Q))  # p(x_t|x_t-1)\n",
    "        mean_state_prediction[t] = np.mean(state_prediction_dist.mean())\n",
    "        std_state_prediction[t] = np.mean(state_prediction_dist.std())\n",
    "        # measurement\n",
    "        measurement_dist = scipy.stats.norm(C * particles[t], np.sqrt(R))\n",
    "        mean_observation[t] = np.mean(measurement_dist.mean())\n",
    "        std_observation[t] = np.mean(measurement_dist.std())\n",
    "\n",
    "        # likelihood\n",
    "        log_obs = measurement_dist.logpdf(y_data[t])\n",
    "        log_state_pred = state_prediction_dist.logpdf(particles[t])\n",
    "        log_state_prop = state_proposal_dist.logpdf(particles[t])\n",
    "        loglikelihood_term = log_obs + log_state_pred - log_state_prop - np.log(nu_weights[t][a_indices]) - np.log(N)\n",
    "        loglikelihood += log_op_exp(loglikelihood_term, np.mean)\n",
    "\n",
    "    nu_weights = np.array(nu_weights)\n",
    "    particles = np.array(particles[:-1])  # remove initial state\n",
    "    mean_filtering = np.array(mean_filtering)\n",
    "    std_filtering = np.array(std_filtering)\n",
    "    mean_state_prediction = np.array(mean_state_prediction)\n",
    "    std_state_prediction = np.array(std_state_prediction)\n",
    "    mean_observation = np.array(mean_observation)\n",
    "    std_observation = np.array(std_observation)\n",
    "    loglikelihood = np.array(loglikelihood)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        nu_weights=nu_weights, particles=particles, mean_filtering=mean_filtering,\n",
    "        std_filtering=std_filtering, mean_state_prediction=mean_state_prediction,\n",
    "        std_state_prediction=std_state_prediction, mean_observation=mean_observation,\n",
    "        std_observation=std_observation, loglikelihood=loglikelihood, ancestor_indices=ancestor_indices,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "initial_particles = np.random.normal(0, 1, N)\n",
    "output = fully_adapted_pf(initial_particles, step_x, C, Q, R, seed=None, theta=1)\n",
    "output.loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(x_data - output.mean_filtering)), np.mean(x_data - output.mean_filtering), np.var(x_data - output.mean_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data - output.mean_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data)\n",
    "plt.plot(output.mean_filtering);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output.std_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_correction(current, proposal, proposal_dist):\n",
    "    proposal_relative = current - proposal + proposal_dist.mean()\n",
    "    current_relative = proposal - current + proposal_dist.mean()\n",
    "    proposal_prob = proposal_dist.logpdf(proposal_relative)\n",
    "    current_prob = proposal_dist.logpdf(current_relative)\n",
    "    return proposal_prob - current_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_metropolis_hastings(n_steps, initial_theta, random_walk_proposal, theta_prior, initial_particle_dist, n_particles, step_x, C, Q, R, verbose=0, seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    current_theta = initial_theta\n",
    "    initial_particles = initial_particle_dist.rvs(n_particles)\n",
    "    output = fully_adapted_pf(initial_particles, step_x, C, Q, R, seed=None, verbose=verbose>1, theta=initial_theta)\n",
    "    current_loglikelihood = output.loglikelihood\n",
    "\n",
    "    thetas = []\n",
    "    loglikelihoods = []\n",
    "    iterator = tqdm(range(n_steps)) if verbose > 0 else range(n_steps)\n",
    "    for m in iterator:\n",
    "        proposed_theta = current_theta + random_walk_proposal.rvs() - random_walk_proposal.mean()\n",
    "\n",
    "        initial_particles = initial_particle_dist.rvs(n_particles)\n",
    "        output = fully_adapted_pf(initial_particles, step_x, C, Q, R, seed=None, verbose=verbose>1, theta=proposed_theta)\n",
    "        proposed_loglikelihood = output.loglikelihood\n",
    "\n",
    "        correction = mh_correction(current_theta, proposed_theta, random_walk_proposal)\n",
    "\n",
    "        proposed_theta_logprob = theta_prior.logpdf(proposed_theta)\n",
    "        current_theta_logprob = theta_prior.logpdf(current_theta)\n",
    "\n",
    "        acceptance = proposed_theta_logprob - current_theta_logprob + proposed_loglikelihood - current_loglikelihood + correction\n",
    "        event = np.log(np.random.uniform(0, 1))\n",
    "        if acceptance > event:\n",
    "            current_theta = proposed_theta\n",
    "            current_loglikelihood = proposed_loglikelihood\n",
    "\n",
    "        thetas.append(current_theta)\n",
    "        loglikelihoods.append(current_loglikelihood)\n",
    "        \n",
    "    return thetas, loglikelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate parameters (infer posterior p(theta|y_data)) using Particle Metropolis Hastings\n",
    "theta_prior = scipy.stats.norm(0, 1)  # p(theta) = N(0, 1)\n",
    "theta_rw_proposal = scipy.stats.norm(0, 1)  # q(theta'|theta) = N(0, 0.1)\n",
    "initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "initial_theta = 0.5  # theta_prior.rvs()\n",
    "N = 500  # Number of APF particles\n",
    "M = 500  # Number of PMH runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas, loglikelihoods = particle_metropolis_hastings(M, initial_theta, theta_rw_proposal, theta_prior, initial_particle_dist, N, step_x, C, Q, R, verbose=1, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thetas), len(loglikelihoods), len(np.unique(loglikelihoods)), round(len(np.unique(loglikelihoods)) / len(loglikelihoods), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas[np.argmax(loglikelihoods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(thetas, bins=50, density=True, label=\"Estimated posterior\")\n",
    "plt.plot([theta, theta], [0, 1], 'r', label='True value')\n",
    "plt.xlabel(\"$\\theta$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(thetas, loglikelihoods);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution modes are biased compared to the true parameters, especially for a low number of timesteps.\n",
    "\n",
    "Increasing the number of timesteps, however, will lead to a more accurate estimate of the posterior mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use a Fully Adapted Particle Filter to implement the Conditional Particle Filter for the Particle Gibbs Sampler.\n",
    "\n",
    "This is slightly more complicated than doing it with the Bootstrap Particle Filter due to the changed order of resample-propagate-weight and the extra set of weights.\n",
    "\n",
    "Hence, we will use the Bootstrap Particle Filter to implement the Conditional Particle Filter for the Particle Gibbs Sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack_genealogy(list_index, list_sample):\n",
    "    \"\"\"Requires initial particle to be at list_sample[-1] and len(list_sample) = len(list_index) + 1\"\"\"\n",
    "    aux_list_index = copy.deepcopy(list_index)\n",
    "    genealogy = [list_sample[-2].reshape(1, -1)]\n",
    "\n",
    "    T = len(list_index)\n",
    "    for t in range(T - 1, -1, -1):  # [4, 3, 2, 1, 0]\n",
    "        genealogy.insert(0, list_sample[t-1][aux_list_index[t]].reshape(1, -1))\n",
    "        aux_list_index[t-1] = aux_list_index[t-1][aux_list_index[t]]\n",
    "\n",
    "    genealogy = np.concatenate(genealogy, axis=0)  # (x_0, x_1, x_2, ..., x_T)\n",
    "    return genealogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_and_var(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return (average, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_bootstrap_pf(initial_particles, reference_trajectory, step_x, C, Q, R, seed=0, verbose=True, **step_kwargs):\n",
    "    \"\"\"Conditional Bootstrap Particle Filter\n",
    "    \n",
    "    Reference trajectory should be given as [x_1, ..., x_T, x_0] i.e. with the initial state last.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = len(initial_particles)\n",
    "    if verbose:\n",
    "        print(f\"Running with {N} particles\")\n",
    "    initial_particles[N-1] = reference_trajectory[-1]  # deterministically set reference trajectory\n",
    "    weights = [None] * T + [np.array([1/N] * N)]\n",
    "    particles = [None] * T + [initial_particles]\n",
    "    mean_filtering = [None] * T\n",
    "    var_filtering = [None] * T\n",
    "    ancestor_indices = [None] * T\n",
    "\n",
    "    iterator = tqdm(range(T)) if verbose else range(T)\n",
    "    for t in iterator:\n",
    "        # RESAMPLE\n",
    "        ancestor_indices[t] = np.random.choice(range(N), p=weights[t-1], replace=True, size=N-1)\n",
    "\n",
    "        # PROPAGATE\n",
    "        # state\n",
    "        fcn = step_x(particles[t-1][ancestor_indices[t]], **step_kwargs)\n",
    "        proposal_dist = scipy.stats.norm(fcn, np.sqrt(Q))\n",
    "        particles[t] = proposal_dist.rvs()\n",
    "        # deterministically set reference trajectory\n",
    "        particles[t] = np.concatenate([particles[t], reference_trajectory[t]])\n",
    "        ancestor_indices[t] = np.concatenate([ancestor_indices[t], [N-1]])\n",
    "        # measurement\n",
    "        fcn = C * particles[t]\n",
    "        measurement_dist = scipy.stats.norm(fcn, np.sqrt(R))\n",
    "\n",
    "        # WEIGHT\n",
    "        log_weights_unnorm = measurement_dist.logpdf(y_data[t])\n",
    "        weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "        weights[t] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "        mean_filtering[t], var_filtering[t] = weighted_mean_and_var(particles[t], weights[t])\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    particles = np.array(particles)\n",
    "    mean_filtering = np.array(mean_filtering)\n",
    "    var_filtering = np.array(var_filtering)\n",
    "    ancestor_indices = np.array(ancestor_indices)\n",
    "\n",
    "    # Sample new reference trajectory\n",
    "    genealogy = backtrack_genealogy(ancestor_indices, particles)\n",
    "    j = np.random.choice(range(N), p=weights[T-1], replace=False, size=1)\n",
    "    reference_trajectory = genealogy[:, j].reshape(-1)  # (T+1, N) -> (T+1,)\n",
    "    reference_trajectory = np.concatenate([reference_trajectory[1:], reference_trajectory[:1]])  # put initial at end\n",
    "\n",
    "    output = SimpleNamespace(\n",
    "        weights=weights,\n",
    "        particles=particles,\n",
    "        mean_filtering=mean_filtering,\n",
    "        var_filtering=var_filtering,\n",
    "        ancestor_indices=ancestor_indices,\n",
    "        genealogy=genealogy,\n",
    "        reference_trajectory=reference_trajectory,\n",
    "        loglikelihood=None,\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "initial_particles = np.random.normal(0, 1, N)\n",
    "initial_reference_trajectory = np.random.normal(5, 1, size=(T + 1, 1))\n",
    "output = conditional_bootstrap_pf(initial_particles, initial_reference_trajectory, step_x, C, Q, R, seed=None, theta=1)\n",
    "output.loglikelihood\n",
    "output.reference_trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_genealogy(genealogy, particles, ancestor_indices, reference_trajectory=None, sampled_trajectory=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "    T = len(genealogy) - 1\n",
    "\n",
    "    ax.plot(list(range(T+1)), genealogy[:,:-1], marker='o', color='tab:red')\n",
    "    ax.plot(list(range(T+1)), genealogy[:,-1:], marker='o', color='tab:red', label=\"Genealogy\")\n",
    "\n",
    "    for t in range(T):\n",
    "        p = np.array([particles[t-1][ancestor_indices[t]], particles[t]])\n",
    "        ax.plot([t, t+1], p, marker='o', color='silver', alpha=0.5)\n",
    "\n",
    "    ax.plot([0, 0], [particles[0,:-1], particles[0,:-1]], marker='o', color='silver', alpha=1)\n",
    "    ax.plot([0, 0], [particles[0,-1:], particles[0,-1:]], marker='o', color='silver', alpha=1, label=\"Particles\")\n",
    "\n",
    "    if reference_trajectory is not None:\n",
    "        # put initial at front\n",
    "        reference_trajectory_plot = np.concatenate([reference_trajectory[-1:], reference_trajectory[:-1]])\n",
    "        ax.plot(reference_trajectory_plot, color=\"tab:blue\", label=\"Reference trajectory\")\n",
    "\n",
    "    if sampled_trajectory is not None:\n",
    "        # put initial at front\n",
    "        sampled_trajectory_plot = np.concatenate([sampled_trajectory[-1:], sampled_trajectory[:-1]])\n",
    "        ax.plot(sampled_trajectory_plot, color=\"tab:green\", label=\"Sampled trajectory\")\n",
    "\n",
    "    ax.legend()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genealogy = output.genealogy\n",
    "particles = output.particles\n",
    "ancestor_indices = output.ancestor_indices\n",
    "\n",
    "plot_genealogy(genealogy, particles, ancestor_indices, initial_reference_trajectory, output.reference_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1047fe110e6526ec8270bc6abda2b9b08acd2c82835ba9522086c3ef7bec77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('seq-mc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
