{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Metropolis Hastings Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mh_correction(current, proposal, proposal_dist):\n",
    "    proposal_relative = current - proposal + proposal_dist.mean()\n",
    "    current_relative = proposal - current + proposal_dist.mean()\n",
    "    proposal_prob = proposal_dist.logpdf(proposal_relative)\n",
    "    current_prob = proposal_dist.logpdf(current_relative)\n",
    "    return proposal_prob - current_prob\n",
    "\n",
    "\n",
    "def metropolis_hastings(initial_point, n_samples, proposal_dist, target_dist_log_prob):\n",
    "    states = [initial_point]\n",
    "    current = initial_point\n",
    "    proposal = initial_point\n",
    "    accept_reject = []\n",
    "\n",
    "    assert not np.isnan(target_dist_log_prob(current)), \"Initial point has NaN log_prob under target distribution\"\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Since the proposal distribution is not symmetric, we need to offset the samples by the mean to not drift\n",
    "        proposal = proposal_dist.rvs() + current - proposal_dist.mean()  # Gaussian random walk proposal\n",
    "        correction = mh_correction(current, proposal, proposal_dist)\n",
    "\n",
    "        prop_log_prob = target_dist_log_prob(proposal)\n",
    "        curr_log_prob = target_dist_log_prob(current)\n",
    "\n",
    "        acceptance = prop_log_prob - curr_log_prob + correction\n",
    "        event = np.log(np.random.uniform(0, 1))\n",
    "        if acceptance > event:\n",
    "            current = proposal\n",
    "        accept_reject.append(float(acceptance > event))\n",
    "\n",
    "        states.append(current)\n",
    "\n",
    "    samples = np.array(states)\n",
    "    return samples, accept_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "proposal_std = 1\n",
    "initial_point = 0\n",
    "\n",
    "proposal_dist = scipy.stats.norm(loc=0, scale=proposal_std)\n",
    "target_dist_unnorm = lambda x: np.sin(x) ** 2 * np.exp(-np.abs(x))\n",
    "target_dist_unnorm_logspace = lambda x: np.log(target_dist_unnorm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, accept_reject = metropolis_hastings(initial_point, n_samples, proposal_dist, target_dist_unnorm_logspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, target_dist_unnorm(x))\n",
    "plt.hist(samples, bins=100, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Gibbs Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler_2d_normal(initial_point, n_samples, means, cov):\n",
    "    samples = []\n",
    "    \n",
    "    x2 = initial_point\n",
    "\n",
    "    cov11 = cov[0, 0]\n",
    "    cov22 = cov[1, 1]\n",
    "    cov12 = cov[0, 1]\n",
    "    cov21 = cov[1, 0]\n",
    "    \n",
    "    mu1 = means[0]\n",
    "    mu2 = means[1]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        mu1_given_2 = mu1 + cov12 / cov22 * (x2 - mu2)\n",
    "        cov1_given_2 = cov11 - (cov12 ** 2) / cov22\n",
    "        x1 = np.random.normal(mu1_given_2, np.sqrt(cov1_given_2))\n",
    "\n",
    "        mu2_given_1 = mu2 + cov21 / cov11 * (x1 - mu1)\n",
    "        cov2_given_1 = cov22 - (cov21 ** 2) / cov11\n",
    "        x2 = np.random.normal(mu2_given_1, np.sqrt(cov2_given_1))\n",
    "        \n",
    "        samples.append([x1, x2])\n",
    "        \n",
    "    samples = np.array(samples)\n",
    "    return samples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = 0\n",
    "n_samples = 10000\n",
    "means = np.array([7, 3])\n",
    "cov = np.array([[0.3, 0.5], [0.5, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gibbs_sampler_2d_normal(initial_point, n_samples, means, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.hist2d(samples[:, 0], samples[:, 1], bins=50, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "(counts, x_bins, y_bins) = np.histogram2d(samples[:, 0], samples[:, 1])\n",
    "ax.contourf(counts, extent=[x_bins[0], x_bins[-1], y_bins[0], y_bins[-1]])\n",
    "ax.scatter(samples[-500:, 0], samples[-500:, 1], s=1, c='r');\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Resampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "x = np.random.normal(0, 1, size=N)\n",
    "w = np.random.uniform(0, 1, size=N)\n",
    "w = w / np.sum(w)\n",
    "    \n",
    "np.mean(x), np.sum(w * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_resampling(w, x, n_strata=None):\n",
    "    n_strata = len(w) if n_strata is None else n_strata\n",
    "    u = (np.arange(n_strata) + np.random.rand(n_strata))/n_strata\n",
    "    bins = np.cumsum(w)\n",
    "    return x[np.digitize(u,bins)]\n",
    "\n",
    "\n",
    "def systematic_resampling(w, x, n_strata=None):\n",
    "    n_strata = len(w) if n_strata is None else n_strata\n",
    "    u = (np.arange(n_strata) + np.random.rand())/n_strata\n",
    "    bins = np.cumsum(w)\n",
    "    return x[np.digitize(u,bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "mean_x = []\n",
    "mean_wx = []\n",
    "mean_multinomial_resampled_x = []\n",
    "mean_stratified_resampling = []\n",
    "mean_systematic_resampling = []\n",
    "\n",
    "for r in tqdm(range(1000)):\n",
    "\n",
    "    x = np.random.normal(0, 1, size=N)\n",
    "    w = np.random.uniform(0, 1, size=N)\n",
    "    w = w / np.sum(w)\n",
    "\n",
    "    mean_x.append(np.mean(x))\n",
    "    mean_wx.append(np.sum(w * x))\n",
    "    \n",
    "    x_resampled = np.random.choice(x, size=N, p=w)\n",
    "    mean_multinomial_resampled_x.append(np.mean(x_resampled))\n",
    "    \n",
    "    x_stratified = stratified_resampling(w, x)\n",
    "    mean_stratified_resampling.append(np.mean(x_stratified))\n",
    "    \n",
    "    x_systematic = systematic_resampling(w, x)\n",
    "    mean_systematic_resampling.append(np.mean(x_systematic))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(mean_x), \\\n",
    "np.var(mean_wx), \\\n",
    "np.var(mean_multinomial_resampled_x), \\\n",
    "np.var(mean_stratified_resampling), \\\n",
    "np.var(mean_systematic_resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Path-space view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_data = pd.read_csv(\"./seOMXlogreturns2012to2014.csv\")\n",
    "\n",
    "observation_data = observation_data.to_numpy()[:, 0]\n",
    "\n",
    "observation_data = observation_data[:50]\n",
    "\n",
    "T = observation_data.shape[0]\n",
    "observation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0.98\n",
    "sigma = 0.16\n",
    "beta = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bootstrap Particle Filter\n",
    "initial_particle_dist = scipy.stats.norm(0, 1)\n",
    "weights = [np.array([1/N] * N)] + [None] * T\n",
    "particles = [initial_particle_dist.rvs(N)] + [None] * T  # draw initial particles\n",
    "mean_observation = [None] * T\n",
    "prediction = [None] * T\n",
    "marginal_filtering = [None] * T\n",
    "ancestor_indices = [None] * T\n",
    "\n",
    "for t in tqdm(range(T)):\n",
    "    # RESAMPLE\n",
    "    a_indices = np.random.choice(range(N), p=weights[t], replace=True, size=N)\n",
    "    ancestor_indices[t] = a_indices\n",
    "\n",
    "    # PROPAGATE\n",
    "    # state\n",
    "    proposal_dist = scipy.stats.norm(phi * particles[t][a_indices], sigma)\n",
    "    particles[t+1] = proposal_dist.rvs()\n",
    "\n",
    "    # measurement\n",
    "    measurement_dist = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(particles[t+1])))\n",
    "    # mean observation\n",
    "    mean_observation[t] = scipy.stats.norm(0, np.sqrt(beta ** 2 * np.exp(np.mean(particles[t+1])))).rvs()\n",
    "\n",
    "    # WEIGHT\n",
    "    log_weights_unnorm = measurement_dist.logpdf(observation_data[t])\n",
    "    weights_unnorm = np.exp(log_weights_unnorm - np.max(log_weights_unnorm))\n",
    "    weights[t+1] = weights_unnorm / np.sum(weights_unnorm)\n",
    "\n",
    "    prediction[t] = np.mean(particles[t])\n",
    "    marginal_filtering[t] = np.sum(weights[t] * particles[t])\n",
    "\n",
    "weights = np.array(weights[:-1])\n",
    "particles = np.array(particles[:-1])\n",
    "mean_observation = np.array(mean_observation)\n",
    "prediction = np.array(prediction)\n",
    "marginal_filtering = np.array(marginal_filtering)\n",
    "ancestor_indices = np.array(ancestor_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestor_indices.shape, particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def backtrack_genealogy(list_index, list_sample):\n",
    "    aux_list_index = copy.deepcopy(list_index)\n",
    "    genealogy = [list_sample[-1].reshape(1,-1)]\n",
    "    \n",
    "    for k in range(len(list_index)-1, 0, -1):\n",
    "        index_previous = aux_list_index[k]\n",
    "        aux_list_index[k-1] = aux_list_index[k-1][index_previous]\n",
    "        genealogy.insert(0, list_sample[k-1][index_previous].reshape(1,-1))\n",
    "  \n",
    "    genealogy = np.concatenate(genealogy,axis =0)\n",
    "    return genealogy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genealogy = backtrack_genealogy(ancestor_indices, particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestor_indices.shape, particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genealogy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.plot(list(range(T)), genealogy, marker='o', color='red')  #, linestyle='--')\n",
    "\n",
    "for t in range(T - 1):\n",
    "    p = np.array([particles[t][ancestor_indices[t+1]], particles[t+1]])\n",
    "    ax.plot([t, t+1], p, marker='o', color='grey', alpha=0.5);  #, linestyle='--')\n",
    "\n",
    "ax.plot([0, 0], [particles[0], particles[0]], marker='o', color='grey', alpha=0.5);  #, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1047fe110e6526ec8270bc6abda2b9b08acd2c82835ba9522086c3ef7bec77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('seq-mc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
